{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Social Computing\n",
    "\n",
    "a.a. 2022-2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attività preliminari"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerie e costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento delle librerie\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartelle di salvataggio\n",
    "data_folder = \"./data\"\n",
    "out_folder = \"./out\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in locale\n",
    "def serialize_json(folder, filename, data):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{folder}/{filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent = 4)\n",
    "        f.close()\n",
    "    print(f\"Data serialized to path: {folder}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura da locale\n",
    "def read_json(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Data read from path: {path}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"No data found at path: {path}\")\n",
    "        return {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si carica il dataset fornito, frammento della combinazione dei dataset [FEVER](https://fever.ai/dataset/fever.html) e [e-FEVER](https://doi.org/10.3929/ethz-b-000453826)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>statement</th>\n",
       "      <th>explanation_human</th>\n",
       "      <th>explanation_model</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51526</td>\n",
       "      <td>Hush (2016 film) was produced by Jason Blum.</td>\n",
       "      <td>Hush (2016 film) was produced by Trevor Macy a...</td>\n",
       "      <td>The evidence says that the film was produced b...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77465</td>\n",
       "      <td>Winter's Tale was released in 1987.</td>\n",
       "      <td>Winter's Tale was released in 2014.</td>\n",
       "      <td>The claim is that Winter's Tale was released i...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166632</td>\n",
       "      <td>Anne Rice was born in the United States of Ame...</td>\n",
       "      <td>Anne Rice was born in New Orleans, Louisiana, ...</td>\n",
       "      <td>The claim is that Anne Rice was born in the Un...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          statement  \\\n",
       "0   51526       Hush (2016 film) was produced by Jason Blum.   \n",
       "1   77465                Winter's Tale was released in 1987.   \n",
       "2  166632  Anne Rice was born in the United States of Ame...   \n",
       "\n",
       "                                   explanation_human  \\\n",
       "0  Hush (2016 film) was produced by Trevor Macy a...   \n",
       "1                Winter's Tale was released in 2014.   \n",
       "2  Anne Rice was born in New Orleans, Louisiana, ...   \n",
       "\n",
       "                                   explanation_model     label  \n",
       "0  The evidence says that the film was produced b...   REFUTES  \n",
       "1  The claim is that Winter's Tale was released i...   REFUTES  \n",
       "2  The claim is that Anne Rice was born in the Un...  SUPPORTS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Si carica e si mostra il dataset fornito\n",
    "df = pd.read_csv(\"./group_9.csv\")\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione degli HITs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con il dataset fornito, vogliamo creare creare dodici HITs aventi le seguenti caratteristiche:\n",
    "\n",
    "1. contiene 3 elementi;\n",
    "2. ogni elemento è dotato di 4 attributi:\n",
    "   1. `id`: identificatore dello statement;\n",
    "   2. `statement`: testo dello statement;\n",
    "   3. `explanation`: testo della spiegazione;\n",
    "   4. `label`: etichetta della spiegazione.\n",
    "3. per due elementi su tre vale che `explanation` = `explanation_model`;\n",
    "4. per un elemento su tre vale che `explanation` = `explanation_human`;\n",
    "5. la posizione dei tre elementi deve essere casuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per l'interpretazione delle righe del dataset\n",
    "def parse_row(row, isGold = False):\n",
    "    parsed = {\n",
    "        \"id\" : row.id,\n",
    "        \"statement\" : row.statement,\n",
    "        \"explanation\" : row.explanation_model,\n",
    "        \"label\" : row.label,\n",
    "        \"isGold\" : isGold\n",
    "    }\n",
    "\n",
    "    # Se è una \"domanda d'oro\", la spiegazione deve essere quella fornita da un essere umano\n",
    "    if isGold:\n",
    "        parsed[\"explanation\"] = row.explanation_human\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data serialized to path: ./data/all_HITs.json\n"
     ]
    }
   ],
   "source": [
    "# Creiamo tutte le possibili permutazioni dei 3 elementi secondo quanto stabilito dalla consegna\n",
    "all_HITs = []\n",
    "\n",
    "# Iteriamo sulle spiegazioni fornite dai modelli di machine learning\n",
    "for model_exp in df.itertuples():\n",
    "    model_HITs = []\n",
    "\n",
    "    # Iteriamo sugli altri \"statements\"\n",
    "    for other_stat in df.itertuples():\n",
    "        if other_stat.id != model_exp.id:\n",
    "            # Inseriamo la versione con la \"domanda d'oro\" per la spiegazione del modello (model_exp)\n",
    "            other_model_HIT = [parse_row(model_exp), parse_row(model_exp, True), parse_row(other_stat)]\n",
    "            # Inseriamo la versione con la \"domanda d'oro\" per questo statement (other_stat)\n",
    "            other_gold_HIT = [parse_row(model_exp), parse_row(other_stat, True), parse_row(other_stat)]\n",
    "\n",
    "            # Riordiniamo pseudo-casualmente gli elementi\n",
    "            random.shuffle(other_model_HIT)\n",
    "            random.shuffle(other_gold_HIT)\n",
    "            # Aggiungiamo le HIT create a quelle relative a questa spiegazione (model_exp)\n",
    "            model_HITs.append(other_model_HIT)\n",
    "            model_HITs.append(other_gold_HIT)\n",
    "    \n",
    "    # Si concatena il tutto a tutti gli HITs possibili\n",
    "    all_HITs += model_HITs\n",
    "\n",
    "# Riposizioniamo gli elementi della lista in maniera pseudo-causale\n",
    "random.shuffle(all_HITs)\n",
    "\n",
    "# Salviamo la lista degli HIT creata\n",
    "serialize_json(data_folder, \"all_HITs.json\", all_HITs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adattamento degli HITs per Crowd_Frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ristrutturiamo gli HITs generati precedentemente in modo da renderli leggibili al software [Crowd_Frame](https://github.com/Miccighel/Crowd_Frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_string(n_chars):\n",
    "    random_string = \"\"\n",
    "    for i in range(n_chars):\n",
    "        # Generiamo un carattere minuscolo\n",
    "        random_integer = random.randint(97, 97 + 26 - 1)\n",
    "        flip_bit = random.randint(0, 1)\n",
    "        # Lo rendiamo casualmente maiuscolo\n",
    "        random_integer = random_integer - 32 if flip_bit == 1 else random_integer\n",
    "        # Concateniamo alla stringa casuale\n",
    "        random_string += chr(random_integer)\n",
    "    return random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from path: ./data/all_HITs.json\n",
      "Data serialized to path: ./out/hits.json\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo gli HITs generati\n",
    "raw_HITs = read_json(data_folder+\"/all_HITs.json\")\n",
    "\n",
    "HITs = []\n",
    "id = 0\n",
    "documents_number = 3\n",
    "\n",
    "for raw_HIT in raw_HITs:\n",
    "    # Creiamo l'HIT definitiva\n",
    "    HIT = {\n",
    "        \"unit_id\" : \"unit_\"+str(id),\n",
    "        \"token_input\" : generate_random_string(10),\n",
    "        \"token_output\" : generate_random_string(10),\n",
    "        \"documents_number\" : documents_number\n",
    "    }\n",
    "    # Aggiungiamo i documenti\n",
    "    documents = []\n",
    "    for element in raw_HIT:\n",
    "        # Discriminiamo le domande d'oro\n",
    "        pre = \"G_\" if element[\"isGold\"] else \"N_\"\n",
    "        # Creiamo il documento con i suoi attributi e lo aggiungiamo\n",
    "        document = {\n",
    "            \"id\" : pre+str(element[\"id\"]),\n",
    "            \"statement\" : element[\"statement\"],\n",
    "            \"label\" : element[\"label\"],\n",
    "            \"explanation\" : element[\"explanation\"]\n",
    "        }\n",
    "        documents.append(document)\n",
    "    # Associamo i documenti riformattati all'HIT\n",
    "    HIT[\"documents\"] = documents\n",
    "    # Aggiungiamo l'HIT all'insieme delle HIT\n",
    "    HITs.append(HIT)\n",
    "    # Incremetiamo il valore dell'id\n",
    "    id += 1\n",
    "\n",
    "# Si esporta il tutto oome file JSON\n",
    "serialize_json(out_folder, \"hits.json\", HITs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "workers_time_id_resp = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_answers.csv\", \n",
    "                         usecols=['worker_id','time_submit_parsed','doc_id','doc_truthfulness-1_value','doc_truthfulness-2_value'])\n",
    "#display(workers_time_id_resp)\n",
    "\n",
    "workers_n = workers_time_id_resp.to_numpy()\n",
    "workers_n_parsed = pd.DataFrame(workers_n, columns=[\"worker_id\", \"time\", \"doc_id\", \"doc_1_val\",\"doc_2_val\"])\n",
    "# otteniamo le risposte per i primi worker e per i secondi workers \n",
    "workers_first_task = workers_n_parsed[[\"worker_id\",\"doc_id\",\"doc_1_val\"]]\n",
    "workers_second_task = workers_n_parsed[[\"worker_id\",\"doc_id\",\"doc_2_val\"]]\n",
    "# abbiamo notato che ci sono stati degli aggiornamenti della query, i valori \n",
    "# non ho trovato una veloce soluzione quindi lo droppo manualmente \n",
    "# problema nella riga 16 in conflitto con la riga 19 -> prevale la riga 19 per via del timestamp \n",
    "# problema nella riga 17 in conflitto con la riga 20 -> prevale la riga 20 (stessa motv)\n",
    "# problema nella riga 18-21-22-23 -> prevale la riga 23 \n",
    "# probelma nella riga 24 in conflitto con la riga 27-28 -> prevale la riga 24\n",
    "# problema nella riga 25 in conflitto con 29 -> prevale la riga 25\n",
    "# problema nella riga 26 in conflitto con 30 -> prevale la riga 26\n",
    "workers_first_task = workers_first_task.drop([16,17,18,21,22,27,28,29,30])\n",
    "workers_first_task = workers_first_task.reset_index(drop=True)\n",
    "# print(workers_first_task)\n",
    "\n",
    "# valutiamo anche le seconde risposte \n",
    "# conflitto con riga 3 -> riga 7 --> vince la riga 7\n",
    "# conflitto con riga 5 -> riga 9 --> vince la riga 5\n",
    "# conflitto con riga 6 -> riga 8 --> vince la riga 8 \n",
    "# conflitto con riga 16 -> riga 19 --> vince la riga 19 \n",
    "# conflitto con riga 17 -> riga 20 --> vince la riga 20 \n",
    "# conflitto con riga 18 - 21 - 22 - 23 -> vince la riga 23 \n",
    "# conflitto con riga 24 - 27 - 28 -> vince la riga 24\n",
    "# conflitto con riga 25 - 29 -> vince la riga 25\n",
    "# conflitto con riga 26 - 30 -> vince la riga 26 \n",
    "workers_second_task = workers_second_task.drop([3,9,6,16,17,18,21,22,27,28,29,30]).reset_index(drop=True)\n",
    "workers_second_task = workers_second_task.set_index('worker_id')\n",
    "# print(workers_second_task)\n",
    "\n",
    "#         risposta worker \n",
    "# doc_id  \n",
    "# table document -> N_166632\n",
    "# ricorda che per il numero di coppie totali possibili -> (n*(n-1))/2\n",
    "def compute_percent_agreement(doc):\n",
    "    doc_reset = doc.reset_index(level=\"worker_id\")\n",
    "\n",
    "    n_rows = doc_reset.shape[0]\n",
    "    total_pairs = (n_rows*(n_rows-1))/2\n",
    "    n_equal_pairs = 0\n",
    "    \n",
    "    for x in range(n_rows): \n",
    "        for y  in range(n_rows):\n",
    "            if(x < y and doc_reset.at[x, \"doc_2_val\"] == doc_reset.at[y, \"doc_2_val\"]): \n",
    "                n_equal_pairs += 1\n",
    "    # percentage agreement \n",
    "    #   numero_coppie_in_accordo\n",
    "    #   ------------------------\n",
    "    #   numero coppie totali \n",
    "    return n_equal_pairs / total_pairs\n",
    "\n",
    "doc_1 = workers_second_task.loc[workers_second_task['doc_id'] == 'N_166632']             # task -> N_166632\n",
    "print(compute_percent_agreement(doc_1)) # proof of work\n",
    "doc_2 = workers_second_task.loc[workers_second_task['doc_id'] == 'G_166632']             # task -> G_166632\n",
    "doc_3 = workers_second_task.loc[workers_second_task['doc_id'] == 'N_51526']              # task -> N_51526\n",
    "doc_4 = workers_second_task.loc[workers_second_task['doc_id'] == 'G_51526']              # task -> G_51526\n",
    "doc_5 = workers_second_task.loc[workers_second_task['doc_id'] == 'N_77465']              # task -> N_77465\n",
    "doc_6 = workers_second_task.loc[workers_second_task['doc_id'] == 'G_77465']              # task -> G_77465\n",
    "\n",
    "# numero di volte che è stato annotato -> prendo tutte le istanze dello stesso task dello stesso worker e calcolo quante sono \n",
    "\n",
    "\n",
    "\n",
    "# Calcolate la percentuale media di testo annotato per ciascuna spiegazione Ordinate le spiegazioni sulla base di tale parametro\n",
    "# in questo caso ci serve tempo inizio tempo fine e task \n",
    "workers_for_time = pd.read_csv('./result/secondo_progetto_social_computing/Dataframe/workers_dimensions_selection.csv', \n",
    "                         usecols=['worker_id','dimension_name','document_id','timestamp_start_parsed','timestamp_end_parsed'])\n",
    "\n",
    "worker_one = workers_for_time.loc[workers_for_time['worker_id'] == 'A348JKD82WQ6Z']     # worker uno A348JKD82WQ6Z\n",
    "worker_two = workers_for_time.loc[workers_for_time['worker_id'] == 'A3OAQZM6Q3YJQ1']    # worker due A3OAQZM6Q3YJQ1\n",
    "worker_tree = workers_for_time.loc[workers_for_time['worker_id'] == 'A1TEEFJDPVEK0L']    # worker tre A1TEEFJDPVEK0L\n",
    "worker_four = workers_for_time.loc[workers_for_time['worker_id'] == 'A3T2NTPGB3KNDS']   # worker 4 A3T2NTPGB3KNDS\n",
    "worker_five = workers_for_time.loc[workers_for_time['worker_id'] == 'AYKZJHEV29ZHL']    # worker 5 AYKZJHEV29ZHL\n",
    "worker_six = workers_for_time.loc[workers_for_time['worker_id'] == 'A2Q51AC4E6I5ZB']    # worker 6 A2Q51AC4E6I5ZB\n",
    "worker_seven = workers_for_time.loc[workers_for_time['worker_id'] == 'A3W16X5D0VGU0E']  # worker 7 A3W16X5D0VGU0E\n",
    "worker_eight = workers_for_time.loc[workers_for_time['worker_id'] == 'A348JKD82WQ6Z']   # worker 8 A2Z4OTGC834F3Y\n",
    "worker_nine = workers_for_time.loc[workers_for_time['worker_id'] == 'A2N1GA8PJDDA6P']   # worker 9 A2N1GA8PJDDA6P\n",
    "worker_ten = workers_for_time.loc[workers_for_time['worker_id'] == 'AYUF9OHXQK2YT']     # worker 10 AYUF9OHXQK2YT\n",
    "\n",
    "# ottenuti i dati di inizio e di fine -> fare la differenza\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6bbff785f7dca992236909a4969e32f59102c4fceccb1043997c81bace8c71c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
