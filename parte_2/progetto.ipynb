{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Social Computing\n",
    "\n",
    "a.a. 2022-2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attività preliminari"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerie e costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento delle librerie\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartelle di salvataggio\n",
    "data_folder = \"./data\"\n",
    "out_folder = \"./out\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in locale\n",
    "def serialize_json(folder, filename, data):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{folder}/{filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent = 4)\n",
    "        f.close()\n",
    "    print(f\"Data serialized to path: {folder}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura da locale\n",
    "def read_json(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Data read from path: {path}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"No data found at path: {path}\")\n",
    "        return {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si carica il dataset fornito, frammento della combinazione dei dataset [FEVER](https://fever.ai/dataset/fever.html) e [e-FEVER](https://doi.org/10.3929/ethz-b-000453826)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>statement</th>\n",
       "      <th>explanation_human</th>\n",
       "      <th>explanation_model</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51526</td>\n",
       "      <td>Hush (2016 film) was produced by Jason Blum.</td>\n",
       "      <td>Hush (2016 film) was produced by Trevor Macy a...</td>\n",
       "      <td>The evidence says that the film was produced b...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77465</td>\n",
       "      <td>Winter's Tale was released in 1987.</td>\n",
       "      <td>Winter's Tale was released in 2014.</td>\n",
       "      <td>The claim is that Winter's Tale was released i...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166632</td>\n",
       "      <td>Anne Rice was born in the United States of Ame...</td>\n",
       "      <td>Anne Rice was born in New Orleans, Louisiana, ...</td>\n",
       "      <td>The claim is that Anne Rice was born in the Un...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          statement  \\\n",
       "0   51526       Hush (2016 film) was produced by Jason Blum.   \n",
       "1   77465                Winter's Tale was released in 1987.   \n",
       "2  166632  Anne Rice was born in the United States of Ame...   \n",
       "\n",
       "                                   explanation_human  \\\n",
       "0  Hush (2016 film) was produced by Trevor Macy a...   \n",
       "1                Winter's Tale was released in 2014.   \n",
       "2  Anne Rice was born in New Orleans, Louisiana, ...   \n",
       "\n",
       "                                   explanation_model     label  \n",
       "0  The evidence says that the film was produced b...   REFUTES  \n",
       "1  The claim is that Winter's Tale was released i...   REFUTES  \n",
       "2  The claim is that Anne Rice was born in the Un...  SUPPORTS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Si carica e si mostra il dataset fornito\n",
    "df = pd.read_csv(\"./group_9.csv\")\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione degli HITs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con il dataset fornito, vogliamo creare creare dodici HITs aventi le seguenti caratteristiche:\n",
    "\n",
    "1. contiene 3 elementi;\n",
    "2. ogni elemento è dotato di 4 attributi:\n",
    "   1. `id`: identificatore dello statement;\n",
    "   2. `statement`: testo dello statement;\n",
    "   3. `explanation`: testo della spiegazione;\n",
    "   4. `label`: etichetta della spiegazione.\n",
    "3. per due elementi su tre vale che `explanation` = `explanation_model`;\n",
    "4. per un elemento su tre vale che `explanation` = `explanation_human`;\n",
    "5. la posizione dei tre elementi deve essere casuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per l'interpretazione delle righe del dataset\n",
    "def parse_row(row, isGold = False):\n",
    "    parsed = {\n",
    "        \"id\" : row.id,\n",
    "        \"statement\" : row.statement,\n",
    "        \"explanation\" : row.explanation_model,\n",
    "        \"label\" : row.label,\n",
    "        \"isGold\" : isGold\n",
    "    }\n",
    "\n",
    "    # Se è una \"domanda d'oro\", la spiegazione deve essere quella fornita da un essere umano\n",
    "    if isGold:\n",
    "        parsed[\"explanation\"] = row.explanation_human\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data serialized to path: ./data/all_HITs.json\n"
     ]
    }
   ],
   "source": [
    "# Creiamo tutte le possibili permutazioni dei 3 elementi secondo quanto stabilito dalla consegna\n",
    "all_HITs = []\n",
    "\n",
    "# Iteriamo sulle spiegazioni fornite dai modelli di machine learning\n",
    "for model_exp in df.itertuples():\n",
    "    model_HITs = []\n",
    "\n",
    "    # Iteriamo sugli altri \"statements\"\n",
    "    for other_stat in df.itertuples():\n",
    "        if other_stat.id != model_exp.id:\n",
    "            # Inseriamo la versione con la \"domanda d'oro\" per la spiegazione del modello (model_exp)\n",
    "            other_model_HIT = [parse_row(model_exp), parse_row(model_exp, True), parse_row(other_stat)]\n",
    "            # Inseriamo la versione con la \"domanda d'oro\" per questo statement (other_stat)\n",
    "            other_gold_HIT = [parse_row(model_exp), parse_row(other_stat, True), parse_row(other_stat)]\n",
    "\n",
    "            # Riordiniamo pseudo-casualmente gli elementi\n",
    "            random.shuffle(other_model_HIT)\n",
    "            random.shuffle(other_gold_HIT)\n",
    "            # Aggiungiamo le HIT create a quelle relative a questa spiegazione (model_exp)\n",
    "            model_HITs.append(other_model_HIT)\n",
    "            model_HITs.append(other_gold_HIT)\n",
    "    \n",
    "    # Si concatena il tutto a tutti gli HITs possibili\n",
    "    all_HITs += model_HITs\n",
    "\n",
    "# Riposizioniamo gli elementi della lista in maniera pseudo-causale\n",
    "random.shuffle(all_HITs)\n",
    "\n",
    "# Salviamo la lista degli HIT creata\n",
    "serialize_json(data_folder, \"all_HITs.json\", all_HITs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adattamento degli HITs per Crowd_Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generazione stringhe casuali\n",
    "def generate_random_string(n_chars):\n",
    "    random_string = \"\"\n",
    "    for i in range(n_chars):\n",
    "        # Generiamo un carattere minuscolo\n",
    "        random_integer = random.randint(97, 97 + 26 - 1)\n",
    "        flip_bit = random.randint(0, 1)\n",
    "        # Lo rendiamo casualmente maiuscolo\n",
    "        random_integer = random_integer - 32 if flip_bit == 1 else random_integer\n",
    "        # Concateniamo alla stringa casuale\n",
    "        random_string += chr(random_integer)\n",
    "    return random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from path: ./data/all_HITs.json\n",
      "Data serialized to path: ./out/hits.json\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo gli HITs generati\n",
    "raw_HITs = read_json(data_folder+\"/all_HITs.json\")\n",
    "\n",
    "HITs = []\n",
    "id = 0\n",
    "documents_number = 3\n",
    "\n",
    "for raw_HIT in raw_HITs:\n",
    "    # Creiamo l'HIT definitiva\n",
    "    HIT = {\n",
    "        \"unit_id\" : \"unit_\"+str(id),\n",
    "        \"token_input\" : generate_random_string(10),\n",
    "        \"token_output\" : generate_random_string(10),\n",
    "        \"documents_number\" : documents_number\n",
    "    }\n",
    "    # Aggiungiamo i documenti\n",
    "    documents = []\n",
    "    for element in raw_HIT:\n",
    "        # Discriminiamo le domande d'oro\n",
    "        pre = \"G_\" if element[\"isGold\"] else \"N_\"\n",
    "        # Creiamo il documento con i suoi attributi e lo aggiungiamo\n",
    "        document = {\n",
    "            \"id\" : pre+str(element[\"id\"]),\n",
    "            \"statement\" : element[\"statement\"],\n",
    "            \"label\" : element[\"label\"],\n",
    "            \"explanation\" : element[\"explanation\"]\n",
    "        }\n",
    "        documents.append(document)\n",
    "    # Associamo i documenti riformattati all'HIT\n",
    "    HIT[\"documents\"] = documents\n",
    "    # Aggiungiamo l'HIT all'insieme delle HIT\n",
    "    HITs.append(HIT)\n",
    "    # Incremetiamo il valore dell'id\n",
    "    id += 1\n",
    "\n",
    "# Si esporta il tutto oome file JSON\n",
    "serialize_json(out_folder, \"hits.json\", HITs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi dei risultati"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent agreement per le variabili categoriali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo del percent-agreement (pa)\n",
    "def get_pair_pa(df, dimension, w1, w2):\n",
    "    dim_name = \"doc_\"+dimension+\"_value\"\n",
    "    # Righe con i dati di w1 e w2\n",
    "    w1_rows = df.loc[df[\"worker_id\"] == w1]\n",
    "    w2_rows = df.loc[df[\"worker_id\"] == w2]\n",
    "    # Documenti valutati da w1 e w2\n",
    "    w1_docs = list(w1_rows[\"doc_id\"])\n",
    "    w2_docs = list(w2_rows[\"doc_id\"])\n",
    "\n",
    "    # Documenti valutati sia da w1 che da w2\n",
    "    docs = [doc for doc in w1_docs if doc in w2_docs]\n",
    "\n",
    "    if (len(docs) == 0):\n",
    "        # Valore indefinito se non ci sono documenti in comune\n",
    "        return -1\n",
    "    else:\n",
    "        total = len(docs)\n",
    "        n_agree = 0\n",
    "        # Righe di w1 e w2 contenenti documenti in comune\n",
    "        w1_rows = w1_rows[w1_rows[\"doc_id\"].isin(docs)]\n",
    "        w2_rows = w2_rows[w2_rows[\"doc_id\"].isin(docs)]\n",
    "\n",
    "        for doc in docs:\n",
    "            # Risposta di w1 per dimension\n",
    "            v1 = list(w1_rows[w1_rows[\"doc_id\"] == doc][dim_name])[0]\n",
    "            # Risposta di w2 per dimension\n",
    "            v2 = list(w2_rows[w2_rows[\"doc_id\"] == doc][dim_name])[0]\n",
    "            if (v1 == v2):\n",
    "                # Se sono d'accordo sul valore, incremento il numero di accordi\n",
    "                n_agree += 1\n",
    "        # Il percent agreement è il numero di accordi su tutti i documenti in comune\n",
    "        return n_agree / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo del percent-agreement (pa) su una dimensione\n",
    "def get_dimension_pa(df, dimension):\n",
    "    # Lista di tutti i worker che hanno risposto\n",
    "    workers = list(df[\"worker_id\"].drop_duplicates())\n",
    "    percent_agreement = []\n",
    "    \n",
    "    for worker_1 in workers:\n",
    "        # Si identifica il primo elemento della coppia\n",
    "        pair_agreement = {\n",
    "            \"worker_x\" : worker_1,\n",
    "        }\n",
    "        for worker_2 in workers:\n",
    "            # Si calcola l'accordo percentuale con il secondo elemento della coppia\n",
    "            pair_agreement[worker_2] = round(get_pair_pa(df, dimension, worker_1, worker_2), 1)\n",
    "        percent_agreement.append(pair_agreement)\n",
    "    \n",
    "    # Si interpretano i dati in un dataframe\n",
    "    pa = pd.DataFrame.from_dict(percent_agreement, orient=\"columns\")\n",
    "    pa = pa.set_index(\"worker_x\")\n",
    "    return pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truthfulness-1 percent agreement\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1TEEFJDPVEK0L</th>\n",
       "      <th>A2N1GA8PJDDA6P</th>\n",
       "      <th>A2Q51AC4E6I5ZB</th>\n",
       "      <th>A2Z4OTGC834F3Y</th>\n",
       "      <th>A348JKD82WQ6Z</th>\n",
       "      <th>A3OAQZM6Q3YJQ1</th>\n",
       "      <th>A3T2NTPGB3KNDS</th>\n",
       "      <th>A3W16X5D0VGU0E</th>\n",
       "      <th>AYKZJHEV29ZHL</th>\n",
       "      <th>AYUF9OHXQK2YT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worker_x</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1TEEFJDPVEK0L</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2N1GA8PJDDA6P</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2Q51AC4E6I5ZB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2Z4OTGC834F3Y</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A348JKD82WQ6Z</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3OAQZM6Q3YJQ1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3T2NTPGB3KNDS</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3W16X5D0VGU0E</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AYKZJHEV29ZHL</th>\n",
       "      <td>50.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AYUF9OHXQK2YT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                A1TEEFJDPVEK0L  A2N1GA8PJDDA6P  A2Q51AC4E6I5ZB  \\\n",
       "worker_x                                                         \n",
       "A1TEEFJDPVEK0L           100.0             0.0             0.0   \n",
       "A2N1GA8PJDDA6P             0.0           100.0             0.0   \n",
       "A2Q51AC4E6I5ZB             0.0             0.0           100.0   \n",
       "A2Z4OTGC834F3Y             0.0             0.0           100.0   \n",
       "A348JKD82WQ6Z              0.0             0.0             0.0   \n",
       "A3OAQZM6Q3YJQ1             0.0           100.0             0.0   \n",
       "A3T2NTPGB3KNDS            -1.0            -1.0           100.0   \n",
       "A3W16X5D0VGU0E             0.0             0.0             0.0   \n",
       "AYKZJHEV29ZHL             50.0            33.3             0.0   \n",
       "AYUF9OHXQK2YT              0.0             0.0             0.0   \n",
       "\n",
       "                A2Z4OTGC834F3Y  A348JKD82WQ6Z  A3OAQZM6Q3YJQ1  A3T2NTPGB3KNDS  \\\n",
       "worker_x                                                                        \n",
       "A1TEEFJDPVEK0L             0.0            0.0             0.0            -1.0   \n",
       "A2N1GA8PJDDA6P             0.0            0.0           100.0            -1.0   \n",
       "A2Q51AC4E6I5ZB           100.0            0.0             0.0           100.0   \n",
       "A2Z4OTGC834F3Y           100.0          100.0           100.0            -1.0   \n",
       "A348JKD82WQ6Z            100.0          100.0            66.7            -1.0   \n",
       "A3OAQZM6Q3YJQ1           100.0           66.7           100.0            -1.0   \n",
       "A3T2NTPGB3KNDS            -1.0           -1.0            -1.0           100.0   \n",
       "A3W16X5D0VGU0E           100.0           66.7            33.3            -1.0   \n",
       "AYKZJHEV29ZHL              0.0            0.0           100.0            -1.0   \n",
       "AYUF9OHXQK2YT             50.0          100.0           100.0            -1.0   \n",
       "\n",
       "                A3W16X5D0VGU0E  AYKZJHEV29ZHL  AYUF9OHXQK2YT  \n",
       "worker_x                                                      \n",
       "A1TEEFJDPVEK0L             0.0           50.0            0.0  \n",
       "A2N1GA8PJDDA6P             0.0           33.3            0.0  \n",
       "A2Q51AC4E6I5ZB             0.0            0.0            0.0  \n",
       "A2Z4OTGC834F3Y           100.0            0.0           50.0  \n",
       "A348JKD82WQ6Z             66.7            0.0          100.0  \n",
       "A3OAQZM6Q3YJQ1            33.3          100.0          100.0  \n",
       "A3T2NTPGB3KNDS            -1.0           -1.0           -1.0  \n",
       "A3W16X5D0VGU0E           100.0            0.0           50.0  \n",
       "AYKZJHEV29ZHL              0.0          100.0            0.0  \n",
       "AYUF9OHXQK2YT             50.0            0.0          100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truthfulness-2 percent agreement\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1TEEFJDPVEK0L</th>\n",
       "      <th>A2N1GA8PJDDA6P</th>\n",
       "      <th>A2Q51AC4E6I5ZB</th>\n",
       "      <th>A2Z4OTGC834F3Y</th>\n",
       "      <th>A348JKD82WQ6Z</th>\n",
       "      <th>A3OAQZM6Q3YJQ1</th>\n",
       "      <th>A3T2NTPGB3KNDS</th>\n",
       "      <th>A3W16X5D0VGU0E</th>\n",
       "      <th>AYKZJHEV29ZHL</th>\n",
       "      <th>AYUF9OHXQK2YT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worker_x</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1TEEFJDPVEK0L</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2N1GA8PJDDA6P</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2Q51AC4E6I5ZB</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2Z4OTGC834F3Y</th>\n",
       "      <td>66.7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A348JKD82WQ6Z</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3OAQZM6Q3YJQ1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3T2NTPGB3KNDS</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3W16X5D0VGU0E</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AYKZJHEV29ZHL</th>\n",
       "      <td>0.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AYUF9OHXQK2YT</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                A1TEEFJDPVEK0L  A2N1GA8PJDDA6P  A2Q51AC4E6I5ZB  \\\n",
       "worker_x                                                         \n",
       "A1TEEFJDPVEK0L           100.0             0.0           100.0   \n",
       "A2N1GA8PJDDA6P             0.0           100.0             0.0   \n",
       "A2Q51AC4E6I5ZB           100.0             0.0           100.0   \n",
       "A2Z4OTGC834F3Y            66.7            50.0             0.0   \n",
       "A348JKD82WQ6Z            100.0             0.0             0.0   \n",
       "A3OAQZM6Q3YJQ1           100.0             0.0             0.0   \n",
       "A3T2NTPGB3KNDS            -1.0            -1.0             0.0   \n",
       "A3W16X5D0VGU0E           100.0             0.0             0.0   \n",
       "AYKZJHEV29ZHL              0.0            66.7             0.0   \n",
       "AYUF9OHXQK2YT            100.0             0.0           100.0   \n",
       "\n",
       "                A2Z4OTGC834F3Y  A348JKD82WQ6Z  A3OAQZM6Q3YJQ1  A3T2NTPGB3KNDS  \\\n",
       "worker_x                                                                        \n",
       "A1TEEFJDPVEK0L            66.7          100.0           100.0            -1.0   \n",
       "A2N1GA8PJDDA6P            50.0            0.0             0.0            -1.0   \n",
       "A2Q51AC4E6I5ZB             0.0            0.0             0.0             0.0   \n",
       "A2Z4OTGC834F3Y           100.0          100.0           100.0            -1.0   \n",
       "A348JKD82WQ6Z            100.0          100.0            66.7            -1.0   \n",
       "A3OAQZM6Q3YJQ1           100.0           66.7           100.0            -1.0   \n",
       "A3T2NTPGB3KNDS            -1.0           -1.0            -1.0           100.0   \n",
       "A3W16X5D0VGU0E           100.0           66.7            33.3            -1.0   \n",
       "AYKZJHEV29ZHL             50.0            0.0             0.0            -1.0   \n",
       "AYUF9OHXQK2YT             50.0          100.0           100.0            -1.0   \n",
       "\n",
       "                A3W16X5D0VGU0E  AYKZJHEV29ZHL  AYUF9OHXQK2YT  \n",
       "worker_x                                                      \n",
       "A1TEEFJDPVEK0L           100.0            0.0          100.0  \n",
       "A2N1GA8PJDDA6P             0.0           66.7            0.0  \n",
       "A2Q51AC4E6I5ZB             0.0            0.0          100.0  \n",
       "A2Z4OTGC834F3Y           100.0           50.0           50.0  \n",
       "A348JKD82WQ6Z             66.7            0.0          100.0  \n",
       "A3OAQZM6Q3YJQ1            33.3            0.0          100.0  \n",
       "A3T2NTPGB3KNDS            -1.0           -1.0           -1.0  \n",
       "A3W16X5D0VGU0E           100.0            0.0           50.0  \n",
       "AYKZJHEV29ZHL              0.0          100.0            0.0  \n",
       "AYUF9OHXQK2YT             50.0            0.0          100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Si carica il csv con le risposte dei worker\n",
    "answers = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_answers.csv\",\n",
    "                        usecols=[\"worker_id\", \"unit_id\", \"doc_id\", \"doc_truthfulness-1_value\", \"doc_explanation-quality_value\", \"doc_truthfulness-2_value\", \"doc_time_elapsed\"])\n",
    "# Si eliminano i duplicati: scegliamo la riga più recente (quella definitiva)\n",
    "answers = answers.loc[answers.reset_index().groupby([\"worker_id\", \"doc_id\"])[\"doc_time_elapsed\"].idxmax()]\n",
    "\n",
    "# Calcoliamo il percent agreement per tutte le dimensioni categoriali\n",
    "for dimension in [\"truthfulness-1\", \"truthfulness-2\"]:\n",
    "    pa = get_dimension_pa(answers, dimension)\n",
    "    pa.to_csv(out_folder+\"/\"+dimension+\"_pa.csv\")\n",
    "    print(dimension+\" percent agreement\")\n",
    "    display(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un lavoratore ha quasi tutti i valori di accordo indefiniti, lo escludo da qui in poi\n",
    "bad_worker = \"A3T2NTPGB3KNDS\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentuale media di testo annotato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta le parole in una stringa\n",
    "def count_words(string):\n",
    "    return len(string.strip('.').split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unisce le tre stringhe\n",
    "def bind_strings(left, center, right):\n",
    "    # Rimuovo gli spazi iniziali o finali le stringhe\n",
    "    left = left.strip(' ')\n",
    "    center = center.strip(' ')\n",
    "    right = right.strip(' ')\n",
    "\n",
    "    # Creo gli spazi tra left-center e center-right se è necessario\n",
    "    sep1 = \" \" if center[0].isalnum() else \"\"\n",
    "    if right != \"\":\n",
    "        sep2 = \" \" if right[0].isalnum() else \"\"\n",
    "    else:\n",
    "        sep2 = \"\"\n",
    "\n",
    "    # Concateno le stringhe ed i separatori\n",
    "    return (left+sep1+center+sep2+right).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Winter's Tale was released in 2014.</th>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hush (2016 film) was produced by Trevor Macy and Jason Blum.</th>\n",
       "      <td>59.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anne Rice was born in New Orleans, Louisiana, which is in the United States of America.</th>\n",
       "      <td>51.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The claim is that Winter's Tale was released in 1987. The evidence states that Winter's Tale is a 1983 novel by Mark Helprin. This is a novel, so it wasn't released in 1987. Therefore, the claim is false.</th>\n",
       "      <td>32.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The claim is that Anne Rice was born in the United States of America. The evidence states that she was born in New Orleans and that New Orleans is a major United States port. Therefore, the claim is true.</th>\n",
       "      <td>30.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The evidence says that the film was produced by Trevor Macy and Jason Blum. The claim says that the film was produced by Jason Blum. So, the answer must be false because the film was produced by Trevor Macy, not just Jason Blum.</th>\n",
       "      <td>30.232558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    annotation_percentage\n",
       "Winter's Tale was released in 2014.                             88.888889\n",
       "Hush (2016 film) was produced by Trevor Macy an...              59.090909\n",
       "Anne Rice was born in New Orleans, Louisiana, w...              51.562500\n",
       "The claim is that Winter's Tale was released in...              32.017544\n",
       "The claim is that Anne Rice was born in the Uni...              30.769231\n",
       "The evidence says that the film was produced by...              30.232558"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carico il file contenente le annotazioni\n",
    "annotations = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_notes.csv\",\n",
    "                        usecols=[\"worker_id\", \"document_index\", \"note_timestamp_created\", \"note_text_left\", \"note_text_current\", \"note_text_right\"])\n",
    "# Rimuovo i duplicati: considero il record più recente\n",
    "annotations = annotations.loc[annotations.reset_index().groupby([\"worker_id\", \"document_index\"])[\"note_timestamp_created\"].idxmax()]\n",
    "# Rimpiazzo eventuali valori indefiniti con la stringa vuota\n",
    "annotations[[\"note_text_left\", \"note_text_current\", \"note_text_right\"]] = annotations[[\"note_text_left\", \"note_text_current\", \"note_text_right\"]].replace(np.nan, \"\")\n",
    "# Rimuovo il lavoratore di infima qualità\n",
    "annotations = annotations.loc[annotations[\"worker_id\"] != bad_worker]\n",
    "avg_annotations_len = {}\n",
    "\n",
    "for index, row in annotations.iterrows():\n",
    "    # Compongo l'intera spiegazione\n",
    "    explanation = bind_strings(row[\"note_text_left\"], row[\"note_text_current\"], row[\"note_text_right\"])\n",
    "    # Conto le parole evidenziate e quelle totali\n",
    "    ann_len = count_words(row[\"note_text_current\"])\n",
    "    exp_len = count_words(explanation)\n",
    "\n",
    "    # Calcolo la percentuale di parole annotate\n",
    "    ann_perc = ann_len / exp_len * 100\n",
    "\n",
    "    if explanation in avg_annotations_len:\n",
    "        # Se la spiegazione è nel vocabolario, aggiungo il valore percentuale di questa annotazione...\n",
    "        avg_annotations_len[explanation].append(ann_perc)\n",
    "    else:\n",
    "        # ... altrimenti creo l'entry per la spiegazione con il valore che ho calcolato\n",
    "        avg_annotations_len[explanation] = [ann_perc]\n",
    "\n",
    "# Calcolo la media delle liste di valori di ogni annotazione\n",
    "for annotation in avg_annotations_len:\n",
    "    percs = avg_annotations_len[annotation]\n",
    "    avg_annotations_len[annotation] = sum(percs) / len(percs)\n",
    "\n",
    "# Ristrutturo tutto in un dataframe ed ordino in ordine decrescente in base al valore percentuale\n",
    "avg_annotations_len = pd.DataFrame.from_dict(avg_annotations_len, orient=\"index\", columns=[\"annotation_percentage\"])\n",
    "avg_annotations_len.sort_values(by=[\"annotation_percentage\"], inplace=True, ascending=False)\n",
    "avg_annotations_len.to_csv(out_folder+\"/avarage_annotations_length.csv\")\n",
    "display(avg_annotations_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numero di volte in cui una spiegazione è stata aggiornata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_51824\\2158687970.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_annotations[\"n_updates\"] = updated_annotations[\"n_updates\"] - 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_updates</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explanation</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The claim is that Winter's Tale was released in 1987. The evidence states that Winter's Tale is a 1983 novel by Mark Helprin. This is a novel, so it wasn't released in 1987. Therefore, the claim is false.</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The evidence says that the film was produced by Trevor Macy and Jason Blum. The claim says that the film was produced by Jason Blum. So, the answer must be false because the film was produced by Trevor Macy, not just Jason Blum.</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The claim is that Anne Rice was born in the United States of America. The evidence states that she was born in New Orleans and that New Orleans is a major United States port. Therefore, the claim is true.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    n_updates\n",
       "explanation                                                  \n",
       "The claim is that Winter's Tale was released in...          2\n",
       "The evidence says that the film was produced by...          2\n",
       "The claim is that Anne Rice was born in the Uni...          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carico il file csv di annotazioni fatte dai worker\n",
    "annotations = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_notes.csv\",\n",
    "                        usecols=[\"worker_id\", \"document_index\", \"note_timestamp_created\", \"note_text_left\", \"note_text_current\", \"note_text_right\"])\n",
    "# Sostituisco i valori non definiti con la stringa vuota\n",
    "annotations[[\"note_text_left\", \"note_text_current\", \"note_text_right\"]] = annotations[[\"note_text_left\", \"note_text_current\", \"note_text_right\"]].replace(np.nan, \"\")\n",
    "# Rimuovo il lavoratore bandito\n",
    "annotations = annotations.loc[annotations[\"worker_id\"] != bad_worker]\n",
    "\n",
    "# Creo una lista in modo che ad ogni worker venga associata la sua spiegazione\n",
    "explanations = []\n",
    "for index, row in annotations.iterrows():\n",
    "    explanation = bind_strings(row[\"note_text_left\"], row[\"note_text_current\"], row[\"note_text_right\"])\n",
    "    explanations.append(explanation)\n",
    "annotations[\"explanation\"] = explanations\n",
    "\n",
    "# Calcolo quante volte è stata aggiunta/modificata un'annotazione\n",
    "annotations = annotations.groupby(by = [\"worker_id\", \"explanation\"]).size().to_frame(\"n_updates\")\n",
    "\n",
    "# Considero solo le annotazioni modificate\n",
    "updated_annotations = annotations[annotations[\"n_updates\"] > 1]\n",
    "# Decrementando di uno il conteggio di aggiunta/modifica si ottiene il conteggio delle modifiche\n",
    "updated_annotations[\"n_updates\"] = updated_annotations[\"n_updates\"] - 1\n",
    "# Sommo per ogni spiegazione, quante volte è stata modificata l'annotazione corrispondente\n",
    "updated_annotations = updated_annotations.groupby([\"explanation\"]).sum()\n",
    "\n",
    "# Ordino in ordine decrescente rispetto al numero di aggiornamenti\n",
    "updated_annotations.sort_values(by=[\"n_updates\"], inplace=True, ascending=False)\n",
    "updated_annotations.to_csv(out_folder+\"/number_annotation_updates.csv\")\n",
    "display(updated_annotations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempo medio impiegato dai worker per valutare ciascun elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_51824\\80866304.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  times = answers.groupby(\"doc_id\").mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_time_elapsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N_166632</th>\n",
       "      <td>291.448333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_51526</th>\n",
       "      <td>162.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_77465</th>\n",
       "      <td>156.171667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G_77465</th>\n",
       "      <td>146.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G_166632</th>\n",
       "      <td>131.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G_51526</th>\n",
       "      <td>124.360000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc_time_elapsed\n",
       "doc_id                    \n",
       "N_166632        291.448333\n",
       "N_51526         162.015000\n",
       "N_77465         156.171667\n",
       "G_77465         146.495000\n",
       "G_166632        131.167500\n",
       "G_51526         124.360000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carico il csv con le risposte dei worker\n",
    "answers = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_answers.csv\",\n",
    "                        usecols=[\"worker_id\", \"doc_id\", \"doc_time_elapsed\"])\n",
    "# Rimuovo le ripetizioni considerando solo le entries più recenti\n",
    "answers = answers.loc[answers.reset_index().groupby([\"worker_id\", \"doc_id\"])[\"doc_time_elapsed\"].idxmax()]\n",
    "# Rimuovo il worker bandito\n",
    "answers = answers.loc[answers[\"worker_id\"] != bad_worker]\n",
    "\n",
    "# Faccio la media della durata per ogni documento\n",
    "times = answers.groupby(\"doc_id\").mean()\n",
    "# Ordino in ordine decrescente sulla base del tempo trascorso a valutare un documento\n",
    "times.sort_values(by=[\"doc_time_elapsed\"], inplace=True, ascending=False)\n",
    "times.to_csv(out_folder+\"/avarage_time.csv\")\n",
    "display(times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6bbff785f7dca992236909a4969e32f59102c4fceccb1043997c81bace8c71c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
