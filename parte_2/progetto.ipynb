{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Social Computing\n",
    "\n",
    "a.a. 2022-2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attività preliminari"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerie e costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento delle librerie\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartelle di salvataggio\n",
    "data_folder = \"./data\"\n",
    "out_folder = \"./out\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in locale\n",
    "def serialize_json(folder, filename, data):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{folder}/{filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent = 4)\n",
    "        f.close()\n",
    "    print(f\"Data serialized to path: {folder}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura da locale\n",
    "def read_json(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Data read from path: {path}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"No data found at path: {path}\")\n",
    "        return {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si carica il dataset fornito, frammento della combinazione dei dataset [FEVER](https://fever.ai/dataset/fever.html) e [e-FEVER](https://doi.org/10.3929/ethz-b-000453826)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>statement</th>\n",
       "      <th>explanation_human</th>\n",
       "      <th>explanation_model</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51526</td>\n",
       "      <td>Hush (2016 film) was produced by Jason Blum.</td>\n",
       "      <td>Hush (2016 film) was produced by Trevor Macy a...</td>\n",
       "      <td>The evidence says that the film was produced b...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77465</td>\n",
       "      <td>Winter's Tale was released in 1987.</td>\n",
       "      <td>Winter's Tale was released in 2014.</td>\n",
       "      <td>The claim is that Winter's Tale was released i...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166632</td>\n",
       "      <td>Anne Rice was born in the United States of Ame...</td>\n",
       "      <td>Anne Rice was born in New Orleans, Louisiana, ...</td>\n",
       "      <td>The claim is that Anne Rice was born in the Un...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          statement  \\\n",
       "0   51526       Hush (2016 film) was produced by Jason Blum.   \n",
       "1   77465                Winter's Tale was released in 1987.   \n",
       "2  166632  Anne Rice was born in the United States of Ame...   \n",
       "\n",
       "                                   explanation_human  \\\n",
       "0  Hush (2016 film) was produced by Trevor Macy a...   \n",
       "1                Winter's Tale was released in 2014.   \n",
       "2  Anne Rice was born in New Orleans, Louisiana, ...   \n",
       "\n",
       "                                   explanation_model     label  \n",
       "0  The evidence says that the film was produced b...   REFUTES  \n",
       "1  The claim is that Winter's Tale was released i...   REFUTES  \n",
       "2  The claim is that Anne Rice was born in the Un...  SUPPORTS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Si carica e si mostra il dataset fornito\n",
    "df = pd.read_csv(\"./group_9.csv\")\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione degli HITs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con il dataset fornito, vogliamo creare creare dodici HITs aventi le seguenti caratteristiche:\n",
    "\n",
    "1. contiene 3 elementi;\n",
    "2. ogni elemento è dotato di 4 attributi:\n",
    "   1. `id`: identificatore dello statement;\n",
    "   2. `statement`: testo dello statement;\n",
    "   3. `explanation`: testo della spiegazione;\n",
    "   4. `label`: etichetta della spiegazione.\n",
    "3. per due elementi su tre vale che `explanation` = `explanation_model`;\n",
    "4. per un elemento su tre vale che `explanation` = `explanation_human`;\n",
    "5. la posizione dei tre elementi deve essere casuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per l'interpretazione delle righe del dataset\n",
    "def parse_row(row, isGold = False):\n",
    "    parsed = {\n",
    "        \"id\" : row.id,\n",
    "        \"statement\" : row.statement,\n",
    "        \"explanation\" : row.explanation_model,\n",
    "        \"label\" : row.label,\n",
    "        \"isGold\" : isGold\n",
    "    }\n",
    "\n",
    "    # Se è una \"domanda d'oro\", la spiegazione deve essere quella fornita da un essere umano\n",
    "    if isGold:\n",
    "        parsed[\"explanation\"] = row.explanation_human\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data serialized to path: ./data/all_HITs.json\n"
     ]
    }
   ],
   "source": [
    "# Creiamo tutte le possibili permutazioni dei 3 elementi secondo quanto stabilito dalla consegna\n",
    "all_HITs = []\n",
    "\n",
    "# Iteriamo sulle spiegazioni fornite dai modelli di machine learning\n",
    "for model_exp in df.itertuples():\n",
    "    model_HITs = []\n",
    "\n",
    "    # Iteriamo sugli altri \"statements\"\n",
    "    for other_stat in df.itertuples():\n",
    "        if other_stat.id != model_exp.id:\n",
    "            # Inseriamo la versione con la \"domanda d'oro\" per la spiegazione del modello (model_exp)\n",
    "            other_model_HIT = [parse_row(model_exp), parse_row(model_exp, True), parse_row(other_stat)]\n",
    "            # Inseriamo la versione con la \"domanda d'oro\" per questo statement (other_stat)\n",
    "            other_gold_HIT = [parse_row(model_exp), parse_row(other_stat, True), parse_row(other_stat)]\n",
    "\n",
    "            # Riordiniamo pseudo-casualmente gli elementi\n",
    "            random.shuffle(other_model_HIT)\n",
    "            random.shuffle(other_gold_HIT)\n",
    "            # Aggiungiamo le HIT create a quelle relative a questa spiegazione (model_exp)\n",
    "            model_HITs.append(other_model_HIT)\n",
    "            model_HITs.append(other_gold_HIT)\n",
    "    \n",
    "    # Si concatena il tutto a tutti gli HITs possibili\n",
    "    all_HITs += model_HITs\n",
    "\n",
    "# Riposizioniamo gli elementi della lista in maniera pseudo-causale\n",
    "random.shuffle(all_HITs)\n",
    "\n",
    "# Salviamo la lista degli HIT creata\n",
    "serialize_json(data_folder, \"all_HITs.json\", all_HITs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6bbff785f7dca992236909a4969e32f59102c4fceccb1043997c81bace8c71c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
