{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Social Computing\n",
    "\n",
    "a.a. 2022-2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attività preliminari"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerie e costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento delle librerie\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartelle di salvataggio\n",
    "data_folder = \"./data\"\n",
    "out_folder = \"./out\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in locale\n",
    "def serialize_json(folder, filename, data):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{folder}/{filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent = 4)\n",
    "        f.close()\n",
    "    print(f\"Data serialized to path: {folder}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura da locale\n",
    "def read_json(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Data read from path: {path}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"No data found at path: {path}\")\n",
    "        return {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si carica il dataset fornito, frammento della combinazione dei dataset [FEVER](https://fever.ai/dataset/fever.html) e [e-FEVER](https://doi.org/10.3929/ethz-b-000453826)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>statement</th>\n",
       "      <th>explanation_human</th>\n",
       "      <th>explanation_model</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51526</td>\n",
       "      <td>Hush (2016 film) was produced by Jason Blum.</td>\n",
       "      <td>Hush (2016 film) was produced by Trevor Macy a...</td>\n",
       "      <td>The evidence says that the film was produced b...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77465</td>\n",
       "      <td>Winter's Tale was released in 1987.</td>\n",
       "      <td>Winter's Tale was released in 2014.</td>\n",
       "      <td>The claim is that Winter's Tale was released i...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166632</td>\n",
       "      <td>Anne Rice was born in the United States of Ame...</td>\n",
       "      <td>Anne Rice was born in New Orleans, Louisiana, ...</td>\n",
       "      <td>The claim is that Anne Rice was born in the Un...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          statement  \\\n",
       "0   51526       Hush (2016 film) was produced by Jason Blum.   \n",
       "1   77465                Winter's Tale was released in 1987.   \n",
       "2  166632  Anne Rice was born in the United States of Ame...   \n",
       "\n",
       "                                   explanation_human  \\\n",
       "0  Hush (2016 film) was produced by Trevor Macy a...   \n",
       "1                Winter's Tale was released in 2014.   \n",
       "2  Anne Rice was born in New Orleans, Louisiana, ...   \n",
       "\n",
       "                                   explanation_model     label  \n",
       "0  The evidence says that the film was produced b...   REFUTES  \n",
       "1  The claim is that Winter's Tale was released i...   REFUTES  \n",
       "2  The claim is that Anne Rice was born in the Un...  SUPPORTS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Si carica e si mostra il dataset fornito\n",
    "df = pd.read_csv(\"./group_9.csv\")\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione degli HITs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con il dataset fornito, vogliamo creare creare dodici HITs aventi le seguenti caratteristiche:\n",
    "\n",
    "1. contiene 3 elementi;\n",
    "2. ogni elemento è dotato di 4 attributi:\n",
    "   1. `id`: identificatore dello statement;\n",
    "   2. `statement`: testo dello statement;\n",
    "   3. `explanation`: testo della spiegazione;\n",
    "   4. `label`: etichetta della spiegazione.\n",
    "3. per due elementi su tre vale che `explanation` = `explanation_model`;\n",
    "4. per un elemento su tre vale che `explanation` = `explanation_human`;\n",
    "5. la posizione dei tre elementi deve essere casuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per l'interpretazione delle righe del dataset\n",
    "def parse_row(row, isGold = False):\n",
    "    parsed = {\n",
    "        \"id\" : row.id,\n",
    "        \"statement\" : row.statement,\n",
    "        \"explanation\" : row.explanation_model,\n",
    "        \"label\" : row.label,\n",
    "        \"isGold\" : isGold\n",
    "    }\n",
    "\n",
    "    # Se è una \"domanda d'oro\", la spiegazione deve essere quella fornita da un essere umano\n",
    "    if isGold:\n",
    "        parsed[\"explanation\"] = row.explanation_human\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data serialized to path: ./data/all_HITs.json\n"
     ]
    }
   ],
   "source": [
    "# Creiamo tutte le possibili permutazioni dei 3 elementi secondo quanto stabilito dalla consegna\n",
    "all_HITs = []\n",
    "\n",
    "# Iteriamo sulle spiegazioni fornite dai modelli di machine learning\n",
    "for model_exp in df.itertuples():\n",
    "    model_HITs = []\n",
    "\n",
    "    # Iteriamo sugli altri \"statements\"\n",
    "    for other_stat in df.itertuples():\n",
    "        if other_stat.id != model_exp.id:\n",
    "            # Inseriamo la versione con la \"domanda d'oro\" per la spiegazione del modello (model_exp)\n",
    "            other_model_HIT = [parse_row(model_exp), parse_row(model_exp, True), parse_row(other_stat)]\n",
    "            # Inseriamo la versione con la \"domanda d'oro\" per questo statement (other_stat)\n",
    "            other_gold_HIT = [parse_row(model_exp), parse_row(other_stat, True), parse_row(other_stat)]\n",
    "\n",
    "            # Riordiniamo pseudo-casualmente gli elementi\n",
    "            random.shuffle(other_model_HIT)\n",
    "            random.shuffle(other_gold_HIT)\n",
    "            # Aggiungiamo le HIT create a quelle relative a questa spiegazione (model_exp)\n",
    "            model_HITs.append(other_model_HIT)\n",
    "            model_HITs.append(other_gold_HIT)\n",
    "    \n",
    "    # Si concatena il tutto a tutti gli HITs possibili\n",
    "    all_HITs += model_HITs\n",
    "\n",
    "# Riposizioniamo gli elementi della lista in maniera pseudo-causale\n",
    "random.shuffle(all_HITs)\n",
    "\n",
    "# Salviamo la lista degli HIT creata\n",
    "serialize_json(data_folder, \"all_HITs.json\", all_HITs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adattamento degli HITs per Crowd_Frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ristrutturiamo gli HITs generati precedentemente in modo da renderli leggibili al software [Crowd_Frame](https://github.com/Miccighel/Crowd_Frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_string(n_chars):\n",
    "    random_string = \"\"\n",
    "    for i in range(n_chars):\n",
    "        # Generiamo un carattere minuscolo\n",
    "        random_integer = random.randint(97, 97 + 26 - 1)\n",
    "        flip_bit = random.randint(0, 1)\n",
    "        # Lo rendiamo casualmente maiuscolo\n",
    "        random_integer = random_integer - 32 if flip_bit == 1 else random_integer\n",
    "        # Concateniamo alla stringa casuale\n",
    "        random_string += chr(random_integer)\n",
    "    return random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from path: ./data/all_HITs.json\n",
      "Data serialized to path: ./out/hits.json\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo gli HITs generati\n",
    "raw_HITs = read_json(data_folder+\"/all_HITs.json\")\n",
    "\n",
    "HITs = []\n",
    "id = 0\n",
    "documents_number = 3\n",
    "\n",
    "for raw_HIT in raw_HITs:\n",
    "    # Creiamo l'HIT definitiva\n",
    "    HIT = {\n",
    "        \"unit_id\" : \"unit_\"+str(id),\n",
    "        \"token_input\" : generate_random_string(10),\n",
    "        \"token_output\" : generate_random_string(10),\n",
    "        \"documents_number\" : documents_number\n",
    "    }\n",
    "    # Aggiungiamo i documenti\n",
    "    documents = []\n",
    "    for element in raw_HIT:\n",
    "        # Discriminiamo le domande d'oro\n",
    "        pre = \"G_\" if element[\"isGold\"] else \"N_\"\n",
    "        # Creiamo il documento con i suoi attributi e lo aggiungiamo\n",
    "        document = {\n",
    "            \"id\" : pre+str(element[\"id\"]),\n",
    "            \"statement\" : element[\"statement\"],\n",
    "            \"label\" : element[\"label\"],\n",
    "            \"explanation\" : element[\"explanation\"]\n",
    "        }\n",
    "        documents.append(document)\n",
    "    # Associamo i documenti riformattati all'HIT\n",
    "    HIT[\"documents\"] = documents\n",
    "    # Aggiungiamo l'HIT all'insieme delle HIT\n",
    "    HITs.append(HIT)\n",
    "    # Incremetiamo il valore dell'id\n",
    "    id += 1\n",
    "\n",
    "# Si esporta il tutto oome file JSON\n",
    "serialize_json(out_folder, \"hits.json\", HITs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi dei risultati"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent agreement per le variabili categoriali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_pa(df, dimension, w1, w2):\n",
    "    dim_name = \"doc_\"+dimension+\"_value\"\n",
    "    w1_rows = df.loc[df[\"worker_id\"] == w1]\n",
    "    w2_rows = df.loc[df[\"worker_id\"] == w2]\n",
    "    w1_docs = list(w1_rows[\"doc_id\"])\n",
    "    w2_docs = list(w2_rows[\"doc_id\"])\n",
    "\n",
    "    docs = [doc for doc in w1_docs if doc in w2_docs]\n",
    "\n",
    "    if (len(docs) == 0):\n",
    "        return -1\n",
    "    else:\n",
    "        total = len(docs)\n",
    "        n_agree = 0\n",
    "        w1_rows = w1_rows[w1_rows[\"doc_id\"].isin(docs)]\n",
    "        w2_rows = w2_rows[w2_rows[\"doc_id\"].isin(docs)]\n",
    "\n",
    "        for doc in docs:\n",
    "            v1 = list(w1_rows[w1_rows[\"doc_id\"] == doc][dim_name])[0]\n",
    "            v2 = list(w2_rows[w2_rows[\"doc_id\"] == doc][dim_name])[0]\n",
    "            if (v1 == v2):\n",
    "                n_agree += 1\n",
    "        \n",
    "        return n_agree / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimension_pa(df, dimension):\n",
    "    workers = list(df[\"worker_id\"].drop_duplicates())\n",
    "    percent_agreement = []\n",
    "    \n",
    "    for worker_1 in workers:\n",
    "        pair_agreement = {\n",
    "            \"worker_x\" : worker_1,\n",
    "        }\n",
    "        for worker_2 in workers:\n",
    "            pair_agreement[worker_2] = get_pair_pa(df, dimension, worker_1, worker_2)\n",
    "        percent_agreement.append(pair_agreement)\n",
    "    \n",
    "    pa = pd.DataFrame.from_dict(percent_agreement, orient=\"columns\")\n",
    "    pa = pa.set_index(\"worker_x\")\n",
    "    return pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truthfulness-1 percent agreement\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1TEEFJDPVEK0L</th>\n",
       "      <th>A2N1GA8PJDDA6P</th>\n",
       "      <th>A2Q51AC4E6I5ZB</th>\n",
       "      <th>A2Z4OTGC834F3Y</th>\n",
       "      <th>A348JKD82WQ6Z</th>\n",
       "      <th>A3OAQZM6Q3YJQ1</th>\n",
       "      <th>A3T2NTPGB3KNDS</th>\n",
       "      <th>A3W16X5D0VGU0E</th>\n",
       "      <th>AYKZJHEV29ZHL</th>\n",
       "      <th>AYUF9OHXQK2YT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worker_x</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1TEEFJDPVEK0L</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2N1GA8PJDDA6P</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2Q51AC4E6I5ZB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2Z4OTGC834F3Y</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A348JKD82WQ6Z</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3OAQZM6Q3YJQ1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3T2NTPGB3KNDS</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3W16X5D0VGU0E</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AYKZJHEV29ZHL</th>\n",
       "      <td>50.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AYUF9OHXQK2YT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                A1TEEFJDPVEK0L  A2N1GA8PJDDA6P  A2Q51AC4E6I5ZB  \\\n",
       "worker_x                                                         \n",
       "A1TEEFJDPVEK0L           100.0        0.000000             0.0   \n",
       "A2N1GA8PJDDA6P             0.0      100.000000             0.0   \n",
       "A2Q51AC4E6I5ZB             0.0        0.000000           100.0   \n",
       "A2Z4OTGC834F3Y             0.0        0.000000           100.0   \n",
       "A348JKD82WQ6Z              0.0        0.000000             0.0   \n",
       "A3OAQZM6Q3YJQ1             0.0      100.000000             0.0   \n",
       "A3T2NTPGB3KNDS            -1.0       -1.000000           100.0   \n",
       "A3W16X5D0VGU0E             0.0        0.000000             0.0   \n",
       "AYKZJHEV29ZHL             50.0       33.333333             0.0   \n",
       "AYUF9OHXQK2YT              0.0        0.000000             0.0   \n",
       "\n",
       "                A2Z4OTGC834F3Y  A348JKD82WQ6Z  A3OAQZM6Q3YJQ1  A3T2NTPGB3KNDS  \\\n",
       "worker_x                                                                        \n",
       "A1TEEFJDPVEK0L             0.0       0.000000        0.000000            -1.0   \n",
       "A2N1GA8PJDDA6P             0.0       0.000000      100.000000            -1.0   \n",
       "A2Q51AC4E6I5ZB           100.0       0.000000        0.000000           100.0   \n",
       "A2Z4OTGC834F3Y           100.0     100.000000      100.000000            -1.0   \n",
       "A348JKD82WQ6Z            100.0     100.000000       66.666667            -1.0   \n",
       "A3OAQZM6Q3YJQ1           100.0      66.666667      100.000000            -1.0   \n",
       "A3T2NTPGB3KNDS            -1.0      -1.000000       -1.000000           100.0   \n",
       "A3W16X5D0VGU0E           100.0      66.666667       33.333333            -1.0   \n",
       "AYKZJHEV29ZHL              0.0       0.000000      100.000000            -1.0   \n",
       "AYUF9OHXQK2YT             50.0     100.000000      100.000000            -1.0   \n",
       "\n",
       "                A3W16X5D0VGU0E  AYKZJHEV29ZHL  AYUF9OHXQK2YT  \n",
       "worker_x                                                      \n",
       "A1TEEFJDPVEK0L        0.000000      50.000000            0.0  \n",
       "A2N1GA8PJDDA6P        0.000000      33.333333            0.0  \n",
       "A2Q51AC4E6I5ZB        0.000000       0.000000            0.0  \n",
       "A2Z4OTGC834F3Y      100.000000       0.000000           50.0  \n",
       "A348JKD82WQ6Z        66.666667       0.000000          100.0  \n",
       "A3OAQZM6Q3YJQ1       33.333333     100.000000          100.0  \n",
       "A3T2NTPGB3KNDS       -1.000000      -1.000000           -1.0  \n",
       "A3W16X5D0VGU0E      100.000000       0.000000           50.0  \n",
       "AYKZJHEV29ZHL         0.000000     100.000000            0.0  \n",
       "AYUF9OHXQK2YT        50.000000       0.000000          100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truthfulness-2 percent agreement\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1TEEFJDPVEK0L</th>\n",
       "      <th>A2N1GA8PJDDA6P</th>\n",
       "      <th>A2Q51AC4E6I5ZB</th>\n",
       "      <th>A2Z4OTGC834F3Y</th>\n",
       "      <th>A348JKD82WQ6Z</th>\n",
       "      <th>A3OAQZM6Q3YJQ1</th>\n",
       "      <th>A3T2NTPGB3KNDS</th>\n",
       "      <th>A3W16X5D0VGU0E</th>\n",
       "      <th>AYKZJHEV29ZHL</th>\n",
       "      <th>AYUF9OHXQK2YT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worker_x</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1TEEFJDPVEK0L</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2N1GA8PJDDA6P</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2Q51AC4E6I5ZB</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2Z4OTGC834F3Y</th>\n",
       "      <td>66.666667</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A348JKD82WQ6Z</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3OAQZM6Q3YJQ1</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3T2NTPGB3KNDS</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3W16X5D0VGU0E</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AYKZJHEV29ZHL</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AYUF9OHXQK2YT</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                A1TEEFJDPVEK0L  A2N1GA8PJDDA6P  A2Q51AC4E6I5ZB  \\\n",
       "worker_x                                                         \n",
       "A1TEEFJDPVEK0L      100.000000        0.000000           100.0   \n",
       "A2N1GA8PJDDA6P        0.000000      100.000000             0.0   \n",
       "A2Q51AC4E6I5ZB      100.000000        0.000000           100.0   \n",
       "A2Z4OTGC834F3Y       66.666667       50.000000             0.0   \n",
       "A348JKD82WQ6Z       100.000000        0.000000             0.0   \n",
       "A3OAQZM6Q3YJQ1      100.000000        0.000000             0.0   \n",
       "A3T2NTPGB3KNDS       -1.000000       -1.000000             0.0   \n",
       "A3W16X5D0VGU0E      100.000000        0.000000             0.0   \n",
       "AYKZJHEV29ZHL         0.000000       66.666667             0.0   \n",
       "AYUF9OHXQK2YT       100.000000        0.000000           100.0   \n",
       "\n",
       "                A2Z4OTGC834F3Y  A348JKD82WQ6Z  A3OAQZM6Q3YJQ1  A3T2NTPGB3KNDS  \\\n",
       "worker_x                                                                        \n",
       "A1TEEFJDPVEK0L       66.666667     100.000000      100.000000            -1.0   \n",
       "A2N1GA8PJDDA6P       50.000000       0.000000        0.000000            -1.0   \n",
       "A2Q51AC4E6I5ZB        0.000000       0.000000        0.000000             0.0   \n",
       "A2Z4OTGC834F3Y      100.000000     100.000000      100.000000            -1.0   \n",
       "A348JKD82WQ6Z       100.000000     100.000000       66.666667            -1.0   \n",
       "A3OAQZM6Q3YJQ1      100.000000      66.666667      100.000000            -1.0   \n",
       "A3T2NTPGB3KNDS       -1.000000      -1.000000       -1.000000           100.0   \n",
       "A3W16X5D0VGU0E      100.000000      66.666667       33.333333            -1.0   \n",
       "AYKZJHEV29ZHL        50.000000       0.000000        0.000000            -1.0   \n",
       "AYUF9OHXQK2YT        50.000000     100.000000      100.000000            -1.0   \n",
       "\n",
       "                A3W16X5D0VGU0E  AYKZJHEV29ZHL  AYUF9OHXQK2YT  \n",
       "worker_x                                                      \n",
       "A1TEEFJDPVEK0L      100.000000       0.000000          100.0  \n",
       "A2N1GA8PJDDA6P        0.000000      66.666667            0.0  \n",
       "A2Q51AC4E6I5ZB        0.000000       0.000000          100.0  \n",
       "A2Z4OTGC834F3Y      100.000000      50.000000           50.0  \n",
       "A348JKD82WQ6Z        66.666667       0.000000          100.0  \n",
       "A3OAQZM6Q3YJQ1       33.333333       0.000000          100.0  \n",
       "A3T2NTPGB3KNDS       -1.000000      -1.000000           -1.0  \n",
       "A3W16X5D0VGU0E      100.000000       0.000000           50.0  \n",
       "AYKZJHEV29ZHL         0.000000     100.000000            0.0  \n",
       "AYUF9OHXQK2YT        50.000000       0.000000          100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_answers.csv\",\n",
    "                        usecols=[\"worker_id\", \"unit_id\", \"doc_id\", \"doc_truthfulness-1_value\", \"doc_explanation-quality_value\", \"doc_truthfulness-2_value\", \"doc_time_elapsed\"])\n",
    "answers = answers.loc[answers.reset_index().groupby([\"worker_id\", \"doc_id\"])[\"doc_time_elapsed\"].idxmax()]\n",
    "\n",
    "for dimension in [\"truthfulness-1\", \"truthfulness-2\"]:\n",
    "    pa = get_dimension_pa(answers, dimension)\n",
    "    pa.to_csv(out_folder+\"/\"+dimension+\"_pa.csv\")\n",
    "    print(dimension+\" percent agreement\")\n",
    "    display(pa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentuale media di testo annotato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(string):\n",
    "    return len(string.strip('.').split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_strings(left, center, right):\n",
    "    left = left.strip(' ')\n",
    "    center = center.strip(' ')\n",
    "    right = right.strip(' ')\n",
    "\n",
    "    sep1 = \" \" if center[0].isalnum() else \"\"\n",
    "    if right != \"\":\n",
    "        sep2 = \" \" if right[0].isalnum() else \"\"\n",
    "    else:\n",
    "        sep2 = \"\"\n",
    "\n",
    "    return (left+sep1+center+sep2+right).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Winter's Tale was released in 2014.</th>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anne Rice was born in New Orleans, Louisiana, which is in the United States of America.</th>\n",
       "      <td>51.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hush (2016 film) was produced by Trevor Macy and Jason Blum.</th>\n",
       "      <td>45.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The claim is that Winter's Tale was released in 1987. The evidence states that Winter's Tale is a 1983 novel by Mark Helprin. This is a novel, so it wasn't released in 1987. Therefore, the claim is false.</th>\n",
       "      <td>32.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The claim is that Anne Rice was born in the United States of America. The evidence states that she was born in New Orleans and that New Orleans is a major United States port. Therefore, the claim is true.</th>\n",
       "      <td>30.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The evidence says that the film was produced by Trevor Macy and Jason Blum. The claim says that the film was produced by Jason Blum. So, the answer must be false because the film was produced by Trevor Macy, not just Jason Blum.</th>\n",
       "      <td>30.232558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    annotation_percentage\n",
       "Winter's Tale was released in 2014.                             88.888889\n",
       "Anne Rice was born in New Orleans, Louisiana, w...              51.562500\n",
       "Hush (2016 film) was produced by Trevor Macy an...              45.454545\n",
       "The claim is that Winter's Tale was released in...              32.017544\n",
       "The claim is that Anne Rice was born in the Uni...              30.769231\n",
       "The evidence says that the film was produced by...              30.232558"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "annotations = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_notes.csv\",\n",
    "                        usecols=[\"worker_id\", \"document_index\", \"note_timestamp_created\", \"note_text_left\", \"note_text_current\", \"note_text_right\"])\n",
    "annotations = annotations.loc[annotations.reset_index().groupby([\"worker_id\", \"document_index\"])[\"note_timestamp_created\"].idxmax()]\n",
    "annotations[[\"note_text_left\", \"note_text_current\", \"note_text_right\"]] = annotations[[\"note_text_left\", \"note_text_current\", \"note_text_right\"]].replace(np.nan, \"\")\n",
    "avg_annotations_len = {}\n",
    "\n",
    "for index, row in annotations.iterrows():\n",
    "    explanation = bind_strings(row[\"note_text_left\"], row[\"note_text_current\"], row[\"note_text_right\"])\n",
    "    ann_len = count_words(row[\"note_text_current\"])\n",
    "    exp_len = count_words(explanation)\n",
    "\n",
    "    ann_perc = ann_len / exp_len * 100\n",
    "\n",
    "    if explanation in avg_annotations_len:\n",
    "        avg_annotations_len[explanation].append(ann_perc)\n",
    "    else:\n",
    "        avg_annotations_len[explanation] = [ann_perc]\n",
    "\n",
    "for annotation in avg_annotations_len:\n",
    "    percs = avg_annotations_len[annotation]\n",
    "    avg_annotations_len[annotation] = sum(percs) / len(percs)\n",
    "\n",
    "avg_annotations_len = pd.DataFrame.from_dict(avg_annotations_len, orient=\"index\", columns=[\"annotation_percentage\"])\n",
    "avg_annotations_len.sort_values(by=[\"annotation_percentage\"], inplace=True, ascending=False)\n",
    "avg_annotations_len.to_csv(out_folder+\"/avarage_annotations_length.csv\")\n",
    "display(avg_annotations_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numero di volte in cui una spiegazione è stata aggiornata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_51824\\1603160887.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_annotations[\"n_updates\"] = updated_annotations[\"n_updates\"] - 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_updates</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explanation</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The claim is that Winter's Tale was released in 1987. The evidence states that Winter's Tale is a 1983 novel by Mark Helprin. This is a novel, so it wasn't released in 1987. Therefore, the claim is false.</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The evidence says that the film was produced by Trevor Macy and Jason Blum. The claim says that the film was produced by Jason Blum. So, the answer must be false because the film was produced by Trevor Macy, not just Jason Blum.</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The claim is that Anne Rice was born in the United States of America. The evidence states that she was born in New Orleans and that New Orleans is a major United States port. Therefore, the claim is true.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    n_updates\n",
       "explanation                                                  \n",
       "The claim is that Winter's Tale was released in...          2\n",
       "The evidence says that the film was produced by...          2\n",
       "The claim is that Anne Rice was born in the Uni...          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "annotations = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_notes.csv\",\n",
    "                        usecols=[\"worker_id\", \"document_index\", \"note_timestamp_created\", \"note_text_left\", \"note_text_current\", \"note_text_right\"])\n",
    "annotations[[\"note_text_left\", \"note_text_current\", \"note_text_right\"]] = annotations[[\"note_text_left\", \"note_text_current\", \"note_text_right\"]].replace(np.nan, \"\")\n",
    "\n",
    "explanations = []\n",
    "for index, row in annotations.iterrows():\n",
    "    explanation = bind_strings(row[\"note_text_left\"], row[\"note_text_current\"], row[\"note_text_right\"])\n",
    "    explanations.append(explanation)\n",
    "\n",
    "annotations[\"explanation\"] = explanations\n",
    "\n",
    "annotations = annotations.groupby(by = [\"worker_id\", \"explanation\"]).size().to_frame(\"n_updates\")\n",
    "\n",
    "updated_annotations = annotations[annotations[\"n_updates\"] > 1]\n",
    "updated_annotations[\"n_updates\"] = updated_annotations[\"n_updates\"] - 1\n",
    "updated_annotations = updated_annotations.groupby([\"explanation\"]).sum()\n",
    "\n",
    "updated_annotations.sort_values(by=[\"n_updates\"], inplace=True, ascending=False)\n",
    "updated_annotations.to_csv(out_folder+\"/number_annotation_updates.csv\")\n",
    "display(updated_annotations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempo medio impiegato dai worker per valutare ciascun elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_51824\\1951336151.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  times = answers.groupby(\"doc_id\").mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_time_elapsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>G_51526</th>\n",
       "      <td>858.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_166632</th>\n",
       "      <td>291.448333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_51526</th>\n",
       "      <td>162.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_77465</th>\n",
       "      <td>156.171667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G_77465</th>\n",
       "      <td>146.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G_166632</th>\n",
       "      <td>131.167500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc_time_elapsed\n",
       "doc_id                    \n",
       "G_51526         858.600000\n",
       "N_166632        291.448333\n",
       "N_51526         162.015000\n",
       "N_77465         156.171667\n",
       "G_77465         146.495000\n",
       "G_166632        131.167500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = pd.read_csv(\"./result/secondo_progetto_social_computing/Dataframe/workers_answers.csv\",\n",
    "                        usecols=[\"worker_id\", \"doc_id\", \"doc_time_elapsed\"])\n",
    "answers = answers.loc[answers.reset_index().groupby([\"worker_id\", \"doc_id\"])[\"doc_time_elapsed\"].idxmax()]\n",
    "times = answers.groupby(\"doc_id\").mean()\n",
    "times.sort_values(by=[\"doc_time_elapsed\"], inplace=True, ascending=False)\n",
    "times.to_csv(out_folder+\"/avarage_time.csv\")\n",
    "display(times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6bbff785f7dca992236909a4969e32f59102c4fceccb1043997c81bace8c71c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
