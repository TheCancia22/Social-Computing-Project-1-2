{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Social Computing\n",
    "\n",
    "a.a. 2022/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attività preliminari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerie e costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import delle librerie utilizzate\n",
    "import os, tweepy, json\n",
    "import networkx as nx \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from pyvis.network import Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartelle di salvataggio\n",
    "data_folder = \"./data\"\n",
    "out_folder = \"./out\"\n",
    "graph_folder = \"./graphs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in locale\n",
    "def serialize_json(folder, filename, data):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    with open(f\"{folder}/{filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent = 4)\n",
    "        f.close()\n",
    "    print(f\"Data serialized to path: {folder}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura da locale\n",
    "def read_json(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Data read from path: {path}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"No data found at path: {path}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credenziali Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento credenziali da JSON\n",
    "api_access = read_json(\"./api_access.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recupero dei follower e dei follower dei follower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupero dei follower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo recuperare, utilizzando la libreria `tweepy`, tutti i follower dell'utente *@KevinRoitero*, corredati delle seguenti informazioni:\n",
    "\n",
    "* attributi di default;\n",
    "* descrizione del profilo;\n",
    "* metriche pubbliche dell'account;\n",
    "* se l'account è protetto\n",
    "\n",
    "Definiamo questi follower come *follower di primo grado*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzamo il client\n",
    "client = tweepy.Client(bearer_token=api_access[\"bearer_token\"])\n",
    "\n",
    "# Definiamo il nostro utente di partenza\n",
    "username = \"KevinRoitero\"\n",
    "all_user_followers = []\n",
    "\n",
    "# Recuperiamo e memorizziamo le informazioni dell'utente\n",
    "response = client.get_user(username = username, user_fields=[\"description\", \"protected\", \"public_metrics\"])\n",
    "user = dict(response.data)\n",
    "\n",
    "# Recuperiamo e memorizziamo i follower dell'utente (con le loro info)\n",
    "response = client.get_users_followers(user[\"id\"], user_fields=[\"description\", \"protected\", \"public_metrics\"], \n",
    "                                      max_results=150) # max_results = 150 perché i follower dell'utente sono nell'ordine di 130\n",
    "for follower in response.data:\n",
    "    all_user_followers.append(dict(follower))\n",
    "\n",
    "# Associamo i follower trovati all'utente di partenza\n",
    "user[\"followers\"] = all_user_followers\n",
    "\n",
    "# Serializziamo su JSON il risultato ottenuto\n",
    "serialize_json(data_folder, \"user_followers.json\", user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggiunta del numero di tweet prodotti nell'ultima settimana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ai follower trovati, si vuole aggiungere il numero di tweet pubblicati nell'ultima settimana. Per avere uniformità, troveremo anche il numero di tweet pubblicati nell'ultima settimana dall'utente *@KevinRoitero*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somma dei numeri di tweet prodotti in un intervallo di tempo\n",
    "def sum_tweets_count(tweet_groups):\n",
    "    sum = 0\n",
    "\n",
    "    for tweet_count in tweet_groups:\n",
    "        sum += tweet_count[\"tweet_count\"]\n",
    "    \n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scegliamo l'utilizzo di un client senza la possibilità di mettersi in attesa perché numero_richieste < 300, \n",
    "# dove 300 è il numero massimo di richieste per l'endpoint get_recent_tweets_count()\n",
    "client = tweepy.Client(bearer_token=api_access[\"bearer_token\"])\n",
    "\n",
    "# Carichiamo i dati dell'utente e quelli dei suoi follower\n",
    "user = read_json(data_folder+\"/user_followers.json\")\n",
    "\n",
    "# Memorizziamo il numero di tweet pubblicati dall'utente nell'ultima settimana\n",
    "response = client.get_recent_tweets_count(query=\"from:\"+user[\"username\"], granularity=\"day\")\n",
    "user[\"last_week_tweets_count\"] = sum_tweets_count(response.data)\n",
    "\n",
    "# Ripetiamo lo stesso per i follower dell'utente\n",
    "for follower in user[\"followers\"]:\n",
    "    if(not follower[\"protected\"]): # non si può accedere ai tweet di un utente 'protected'\n",
    "        response = client.get_recent_tweets_count(query=\"from:\"+follower[\"username\"], granularity=\"day\")\n",
    "        follower[\"last_week_tweets_count\"] = sum_tweets_count(response.data)\n",
    "\n",
    "# Serializziamo su JSON il risultato ottenuto\n",
    "serialize_json(data_folder, \"followers_last_week_tweets.json\", user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupero dei follower dei follower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ciascun follower di *@KevinRoitero* avente almeno 1 follower e non `protected`, vogliamo scaricare le seguenti informazioni:\n",
    "\n",
    "* attributi di default;\n",
    "* descrizione del profilo;\n",
    "* metriche pubbliche dell'account;\n",
    "* se l'account è protetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementeremo due versioni: una in cui verranno scaricati al più 1000 follower per ogni follower di *@KevinRoitero* ed una in cui verranno invece scaricati tutti i follower dei follower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chiameremo questi *follower di secondo grado*. Se un follower di secondo grado è anche di primo grado, verrà da noi considerato come follower di primo grado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versione con al più mille follower di follower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci riferiremo da qui in poi a questa versione dei dati come ad una versione *parziale*, in quanto non presenterà lo stesso livello di completezza dell'informazione della sua controparte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imponiamo al client di attendere nel caso di raggiungimento del limite delle richieste\n",
    "client = tweepy.Client(bearer_token=api_access[\"bearer_token\"], wait_on_rate_limit=True)\n",
    "\n",
    "# Carichiamo i dati dell'utente e quelli dei suoi follower\n",
    "user = read_json(data_folder+\"/followers_last_week_tweets.json\")\n",
    "\n",
    "# Scarichiamo al più 1000 follower per ogni follower dell'utente\n",
    "for follower in user[\"followers\"]:\n",
    "    follower[\"followers\"] = []\n",
    "\n",
    "    # Non si possono scaricare i dettagli dei follower di profili privati\n",
    "    # (e non ha senso scaricare i follower di utenti che non ne hanno)\n",
    "    if (not follower[\"protected\"] and follower[\"public_metrics\"][\"followers_count\"] > 0):\n",
    "        some_followers = client.get_users_followers(\n",
    "            id = follower[\"id\"], \n",
    "            user_fields = [\"description\", \"protected\", \"public_metrics\"],\n",
    "            max_results = 1000\n",
    "        )\n",
    "\n",
    "        # Aggiungo il parse del follower ai dati finali\n",
    "        for ff in some_followers.data:\n",
    "            follower[\"followers\"].append(dict(ff))\n",
    "\n",
    "# Serializziamo il risultato\n",
    "serialize_json(data_folder, \"user_partial.json\", user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versione con tutti i follower di follower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di partire con il download, cerchiamo di farci un'idea dei volumi di dati che dovremo scaricare (e dei tempi di attesa che ne conseguono)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo il file con le informazioni sull'utente\n",
    "user = read_json(data_folder+\"/followers_last_week_tweets.json\")\n",
    "\n",
    "# Se il follower non ha un profilo privato, vediamo quanti follower ha\n",
    "fof_quantities = []\n",
    "for follower in user[\"followers\"]:\n",
    "    if (not follower[\"protected\"]):\n",
    "        fof_quantities.append(follower[\"public_metrics\"][\"followers_count\"])\n",
    "\n",
    "# Organizziamo tutto in tabelle riassuntive\n",
    "fof_df = pd.DataFrame(fof_quantities, columns=[\"f_count\"])\n",
    "print(\"Numero dei follower di tutti i follower\")\n",
    "display(fof_df.describe())\n",
    "\n",
    "print(\"Numero di seguaci dei follower con più di 1000 follower\")\n",
    "display(fof_df.loc[fof_df[\"f_count\"] > 1000].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'approccio che è stato seguito è quello di un download *parallelo*, ossia ciascun collaboratore al progetto si è preso in carico di scaricare una porzione di follower dei follower di *@KevinRoitero*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imponiamo al client di attendere nel caso di raggiungimento del limite delle richieste\n",
    "client = tweepy.Client(bearer_token=api_access[\"bearer_token\"], wait_on_rate_limit=True)\n",
    "\n",
    "# Carichiamo i dati dell'utente e quelli dei suoi follower\n",
    "user = read_json(data_folder+\"/followers_last_week_tweets.json\")\n",
    "\n",
    "all_followers = user[\"followers\"]\n",
    "\n",
    "# Determiniamo il segmento di follower di competenza di questo client\n",
    "f_num = user[\"public_metrics\"][\"followers_count\"]\n",
    "start = f_num*api_access[\"id\"] // 4\n",
    "end =  f_num*(api_access[\"id\"]+1) // 4\n",
    "\n",
    "# Memorizziamo solo i follower di secondo grado nel segmento del client\n",
    "interval_followers = []\n",
    "\n",
    "# Scarichiamo i follower di secondo grado nell'intervallo del collaboratore\n",
    "for i in range(start, end):\n",
    "    # Consideriamo l'i-esimo follower\n",
    "    follower = all_followers[i]\n",
    "    follower[\"followers\"] = []\n",
    "\n",
    "    # Se l'i-esimo follower non è protetto e ha dei follower, li scarichiamo tutti\n",
    "    if (not follower[\"protected\"] and follower[\"public_metrics\"][\"followers_count\"] > 0):\n",
    "        for all_follower_followers in tweepy.Paginator(\n",
    "            client.get_users_followers,\n",
    "            id = follower[\"id\"],user_fields=[\"description\",\n",
    "            \"protected\", \"public_metrics\"],\n",
    "            max_results = 1000 # chiediamo al più 1000 follower per ogni pagina di risposta\n",
    "            ):\n",
    "\n",
    "            parsed_followers = []\n",
    "            \n",
    "            # Interpretiamo ogni oggetto User nella pagina\n",
    "            for ff in all_follower_followers.data:\n",
    "                parsed_follower = {\n",
    "                    \"id\" : ff[\"id\"],\n",
    "                    \"public_metrics\" : ff[\"public_metrics\"],\n",
    "                    \"description\" : ff[\"description\"],\n",
    "                    \"name\" : ff[\"name\"],\n",
    "                    \"protected\" : ff[\"protected\"],\n",
    "                    \"username\" : ff[\"username\"]\n",
    "                }\n",
    "                parsed_followers.append(parsed_follower)\n",
    "            \n",
    "            # Aggiungiamo i dettagli dei profili interpretati all'i-esimo follower\n",
    "            follower[\"followers\"] += parsed_followers\n",
    "    \n",
    "    # Aggiungiamo il follower (arricchito dei suoi follower) nella lista di follower di competenza del collaboratore\n",
    "    interval_followers.append(follower)\n",
    "\n",
    "# Salviamo la porzione di follower scaricata\n",
    "serialize_json(data_folder, \"f_of_f_\"+str(api_access[\"id\"])+\".json\", interval_followers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si uniscono ora i file generati dai diversi collaboratori in un unico file JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo il JSON con le informazioni su @KevinRoitero\n",
    "user = read_json(data_folder+\"/followers_last_week_tweets.json\")\n",
    "followers = []\n",
    "\n",
    "for i in range(4):\n",
    "    # Carichiamo i follower individuati dal collaboratore i\n",
    "    partial_followers = read_json(data_folder+\"/f_of_f_\"+str(i)+\".json\")\n",
    "    \n",
    "    # Carichiamo all'elenco totale dei follower\n",
    "    followers += partial_followers\n",
    "\n",
    "# Aggiorniamo i dati sui follower di @KevinRoitero\n",
    "user[\"followers\"] = followers\n",
    "\n",
    "# Serializziamo il risultato ottenuto\n",
    "serialize_json(data_folder, \"user_full.json\", user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riformattazione del JSON finale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al fine di essere conformi alle specifiche della consegna, si decide di ristrutturare (ossia modificarne la presentazione, pur mantenendo invariate le informazioni al suo interno) i JSON `user_partial.json` e `user_full.json` definiti in precedenza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il formato che è stato ritenuto come più opportuno, sia per un'efficienza di archiviazione che di computazione (importante per le sezioni successive), è il seguente:\n",
    "```json\n",
    "{\n",
    "    ...\n",
    "    id (int) : {\n",
    "        \"name\" : str,\n",
    "        \"username\" : str,\n",
    "        \"description\" : int,\n",
    "        \"public_metrics\" : {\n",
    "            \"followers_count\" : int,\n",
    "            \"following_count\" : int,\n",
    "            \"tweet_count\" : int,\n",
    "            \"listed_count\" : int\n",
    "        },\n",
    "        \"protected\" : bool,\n",
    "        (\"last_week_tweets_count\" :int,)\n",
    "        (\"followers\" : [\n",
    "            ...\n",
    "            f_id (int),\n",
    "            ...\n",
    "        ])\n",
    "    }\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dove `id` rappresenta l'identificativo univoco di un utente, le cui informazioni sono state scaricate nei passi precedenti, ed `f_id` rappresenta l'identificativo di un follower di `id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È doveroso far notare che, per come sono stati scaricati gli utenti, solamente coloro che o sono *@KevinRoitero* o sono suoi follower (non `protected`) presentano il campo `last_week_tweets_count` ed il campo `followers` (da cui le parentesi tonde nello schema)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chiameremo il formato prodotto dai download *formato ad albero* ed il formato prodotto da questa conversione *formato a tabella hash*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo una funzione che esegue una normalizzazione di un singolo utente\n",
    "def normalize_into(user, root_distance, destination):\n",
    "    # Normalizziamo l'utente considerato\n",
    "    normalized = {\n",
    "        \"name\" : user[\"name\"],\n",
    "        \"username\" : user[\"username\"],\n",
    "        \"description\" : user[\"description\"],\n",
    "        \"public_metrics\" : user[\"public_metrics\"],\n",
    "        \"protected\" : user[\"protected\"]\n",
    "    }\n",
    "\n",
    "    # Se user non è un follower di secondo grado e non è protetto, si possono definire i dettagli aggiuntivi\n",
    "    if (root_distance < 2 and not user[\"protected\"]):\n",
    "        # Estraiamo le ids dei follower dell'utente\n",
    "        f_ids = []\n",
    "        for f in user[\"followers\"]:\n",
    "            f_ids.append(f[\"id\"])\n",
    "\n",
    "        # Normalizziamo le informazioni restanti\n",
    "        normalized[\"last_week_tweets_count\"] = user[\"last_week_tweets_count\"]\n",
    "        normalized[\"followers\"] = f_ids\n",
    "    \n",
    "    # Se l'utente non è già definito in 'destination', lo aggiungiamo\n",
    "    if user[\"id\"] not in destination:\n",
    "        destination[user[\"id\"]] = normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dato un json strutturato ad albero, lo trasforma in un json a tabella hash\n",
    "def normalize_json(path, final_name):\n",
    "    # Carichiamo il JSON totale, non formattato\n",
    "    root_user = read_json(path)\n",
    "\n",
    "    nodes = {} # JSON finale\n",
    "\n",
    "    # Normalizziamo @KevinRoitero\n",
    "    normalize_into(user = root_user, root_distance=0, destination = nodes)\n",
    "\n",
    "    # Facciamo la stessa cosa per follower i suoi follower\n",
    "    for follower in root_user[\"followers\"]:\n",
    "        normalize_into(user = follower, root_distance = 1, destination = nodes)\n",
    "\n",
    "    # ... e per i follower dei follower\n",
    "    for follower in root_user[\"followers\"]:\n",
    "        if (not follower[\"protected\"]):\n",
    "            for ff in follower[\"followers\"]:\n",
    "                normalize_into(user = ff, root_distance = 2, destination = nodes)\n",
    "\n",
    "    # Serializziamo il JSON ben formattato\n",
    "    serialize_json(data_folder, final_name+\".json\", nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riformattazione JSON parziale\n",
    "normalize_json(path=data_folder+\"/user_partial.json\", final_name=\"some_users_final\")\n",
    "\n",
    "# Riformattazione JSON totale\n",
    "normalize_json(path=data_folder+\"/user_full.json\", final_name=\"all_users_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideriamo che, per come è stato costruito, i file `*_users_final.json` presenta una struttura ordinata:\n",
    "\n",
    "1. Al primo posto vi sonoi dettagli sul profilo di *@KevinRoitero*;\n",
    "2. dal secondo posto fino al 134-esimo vi sono i follower di *@KevinRoitero*;\n",
    "3. dal 135-esimo posto in poi vi sono i follower dei follower di *@KevinRoitero*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generazione dei grafi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generazione del grafo della rete sociale diretta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo generare un grafo che abbia come nodi *@KevinRoitero* ed i suoi follower, ognuno di questi deve rispettare le seguenti caratteristiche:\n",
    "\n",
    "* il suo `id` deve essere uguale all'`id` del profilo utente;\n",
    "* deve avere come attributi:\n",
    "    * lo username;\n",
    "    * la descrizione;\n",
    "    * il numero di follower del profilo.\n",
    "\n",
    "Infine, vogliamo aggiungere archi $(v,w)$ tra due nodi $v$ e $w$ se e solo se il profilo corrispondente a $v$ è follower del profilo corrispondente a $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un grafo diretto dove un arco tra nodi indica una relazione di \"following\"\n",
    "def create_following_graph(users, name):\n",
    "    # Creiamo un grafo diretto vuoto\n",
    "    following_graph = nx.DiGraph()\n",
    "\n",
    "    # Inseriamo @KevinRoitero\n",
    "    main_id = list(users.keys())[0]\n",
    "    main = users[main_id]\n",
    "    following_graph.add_node(\n",
    "        int(main_id),\n",
    "        username = main[\"username\"],\n",
    "        description = main[\"description\"],\n",
    "        followers_count = main[\"public_metrics\"][\"followers_count\"]\n",
    "    )\n",
    "\n",
    "    # Inseriamo i follower\n",
    "    for id in main[\"followers\"]:\n",
    "        user = users[str(id)]\n",
    "        following_graph.add_node(\n",
    "            id,\n",
    "            username = user[\"username\"], \n",
    "            description = user[\"description\"],\n",
    "            followers_count = user[\"public_metrics\"][\"followers_count\"]\n",
    "        )\n",
    "\n",
    "    # Aggiungiamo gli archi tra i nodi\n",
    "    for w in following_graph.nodes:\n",
    "        w_key = str(w)\n",
    "        # Sappiamo le relazioni di following solo per gli utenti con profilo pubblico\n",
    "        if not users[w_key][\"protected\"]:\n",
    "            for v in users[w_key][\"followers\"]:\n",
    "                if (v in following_graph.nodes):\n",
    "                    following_graph.add_edge(v, w)\n",
    "\n",
    "    # Serializziamo il grafo finale\n",
    "    serialize_json(graph_folder, name+\"_following_graph.json\", nx.node_link_data(following_graph))\n",
    "\n",
    "    return following_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i due json a tabella di hash\n",
    "partial_users = read_json(data_folder+\"/some_users_final.json\")\n",
    "full_users = read_json(data_folder+\"/all_users_final.json\")\n",
    "\n",
    "# Grafo parziale\n",
    "partial_fg = create_following_graph(partial_users, \"some_users\")\n",
    "# Grafo totale\n",
    "full_fg = create_following_graph(full_users, \"all_users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generazione grafo con preferential attachment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generiamo un secondo grafo a partire da quello definito in precedenza, nel seguente modo:\n",
    "\n",
    "1. convertiamo `*_following_graph` in grafo indiretto;\n",
    "2. a tale grafo indiretto aggiungiamo dei nodi, utilizzando il metodo del preferential attachment, tali che:\n",
    "    - il numero di nodi aggiunti è uguale al numero di nodi già presenti nel grafo;\n",
    "    - ogni nodo aggiunto ha due archi uscenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non essendo specificati ulteriori dettagli nella consegna, ci riconduciamo al modello più semplice utilizzando un attaccamento preferenziale lineare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_preferential_graph(directed_graph, name):\n",
    "    # Creiamo il grafo indiretto intermedio\n",
    "    number_of_nodes = directed_graph.number_of_nodes()\n",
    "    undirected_graph = directed_graph.to_undirected()\n",
    "\n",
    "    # Aggiungiamo i nodi utilizzando il preferential attachment lineare\n",
    "    preferential_graph = nx.barabasi_albert_graph(number_of_nodes*2, 2, initial_graph = undirected_graph)\n",
    "\n",
    "    # Serializzazione del grafo\n",
    "    serialize_json(graph_folder, name+\"_preferential_graph.json\", nx.node_link_data(preferential_graph))\n",
    "    return preferential_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i grafi definiti in precedenza\n",
    "partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "\n",
    "# Grafo \"preferenziale\" parziale\n",
    "partial_pg = convert_to_preferential_graph(partial_fg, \"some_users\")\n",
    "# Grafo \"preferenziale\" totale\n",
    "full_pg = convert_to_preferential_graph(full_fg, \"all_users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi dei grafi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzazione interattiva dei due grafi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizzando la libreria `pyivs`, verrà creata una visualizzazione interattiva del grafo dei follower e del grafo \"prefernziale\" (`preferential_graph`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo una funzione per visualizzare interattivamente un grafo\n",
    "def show_graph(graph, graph_name):\n",
    "    # Rappresentiamo gli archi come frecce se il grafo è diretto\n",
    "    if (nx.is_directed(graph)):\n",
    "        network = Network(height=\"720px\", directed=True, bgcolor=\"#222222\", font_color=\"white\") # width = 100% default\n",
    "    else:\n",
    "        network = Network(height=\"720px\", bgcolor=\"#222222\", font_color=\"white\") # width = 100% default\n",
    "    network.barnes_hut() # layout\n",
    "    network.from_nx(graph)\n",
    "\n",
    "    # Aggiungiamo le etichette ai nodi\n",
    "    neighbours = network.get_adj_list()\n",
    "    for node in network.nodes:\n",
    "        node[\"value\"] = len(neighbours[node[\"id\"]])\n",
    "    \n",
    "    # Mostriamo la visualizzazione interattiva\n",
    "    network.show(out_folder+\"/\"+graph_name+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i due grafi diretti\n",
    "partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "\n",
    "# Visualizziamo i grafi\n",
    "show_graph(partial_fg, \"partial_fg\")\n",
    "show_graph(full_fg, \"full_fg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i due grafi indiretti\n",
    "partial_pg = nx.node_link_graph(read_json(graph_folder+\"/some_users_preferential_graph.json\"))\n",
    "full_pg = nx.node_link_graph(read_json(graph_folder+\"/all_users_preferential_graph.json\"))\n",
    "\n",
    "# Visualizziamo i grafi\n",
    "show_graph(partial_pg, \"partial_pg\")\n",
    "show_graph(full_pg, \"full_pg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzazione dei grafi attraverso nodi di dimensione variabile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vogliono disegnare i due grafi definiti nelle sezioni precedenti in modo che la dimensione dei nodi raffigurati sia direttamente proporzionale:\n",
    "\n",
    "- all'in-degree (grado in entrata) per il grafo diretto\n",
    "- al degree (grado) per il grafo indiretto\n",
    "\n",
    " Si userà l'algoritmo di [Fruchterman-Reingold](https://github.com/gephi/gephi/wiki/Fruchterman-Reingold) per la distribuzione dei nodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disegna un grafo in cui la dimensione dei nodi è direttamente proporzionale all'(in-)degree\n",
    "def draw_graph(graph):\n",
    "    node_size = []\n",
    "    k = 4 # costante di \"ingrandimento\" dei nodi\n",
    "    # A seconda della natura del grafo, scegliamo quale metrica usare\n",
    "    if nx.is_directed(graph):\n",
    "        for node in graph.nodes():\n",
    "            node_size.append(graph.in_degree(node)*k)\n",
    "    else:\n",
    "        for node in graph.nodes():\n",
    "            node_size.append(graph.degree(node)*k)\n",
    "\n",
    "    # Disegnamo il grafo\n",
    "    plot = nx.draw_networkx(\n",
    "        graph, \n",
    "        pos = nx.spring_layout(graph), # algoritmo di Fruchterman-Reingold\n",
    "        node_color = '#A0CBE2',\n",
    "        edge_color = (0,0,0,0.1), # gli archi sono trasparenti al fine di far risaltare i nodi\n",
    "        with_labels = False,\n",
    "        node_size = node_size\n",
    "    )\n",
    "\n",
    "    return plot\n",
    "\n",
    "# Mette a confronto due grafi i cui nodi hanno dimensione proporzionale al grado\n",
    "def compare_degrees(partial_G, total_G, name):\n",
    "    plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Disegnamo il grafo parziale\n",
    "    plt.subplot(2, 1, 1)\n",
    "    draw_graph(partial_G)\n",
    "    plt.title(\"Grafo parziale\")\n",
    "\n",
    "    # Disegnamo il grafo totale\n",
    "    plt.subplot(2, 1, 2)\n",
    "    draw_graph(total_G)\n",
    "    plt.title(\"Grafo totale\")\n",
    "\n",
    "    # Salviamo e mostriamo quanto disegnato\n",
    "    plt.savefig(out_folder+\"/\"+name+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i due grafi diretti\n",
    "partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "\n",
    "# Disegnamo i due grafi\n",
    "compare_degrees(partial_fg, full_fg, \"following_graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si caricano i due grafi indiretti\n",
    "partial_pg = nx.node_link_graph(read_json(graph_folder+\"/some_users_preferential_graph.json\"))\n",
    "full_pg = nx.node_link_graph(read_json(graph_folder+\"/all_users_preferential_graph.json\"))\n",
    "\n",
    "# Disegnamo i due grafi\n",
    "compare_degrees(partial_pg, full_pg, \"preferential_graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci interessa visualizzare in maniera più chiara i gradi dei grafi: disegnamo quindi la distribuzione dei gradi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restituisce una lista di frequenze (non razionali, intere)\n",
    "def degree_histogram_directed(G, in_degree=False, out_degree=False):\n",
    "    nodes = G.nodes()\n",
    "    # Popoliamo la lista con tutti i (in/out-)gradi dei nodi\n",
    "    if in_degree:\n",
    "        in_degree = dict(G.in_degree())\n",
    "        degseq=[in_degree.get(k,0) for k in nodes]\n",
    "    elif out_degree:\n",
    "        out_degree = dict(G.out_degree())\n",
    "        degseq=[out_degree.get(k,0) for k in nodes]\n",
    "    else:\n",
    "        degseq=[v for k, v in G.degree()]\n",
    "\n",
    "    dmax=max(degseq)+1\n",
    "    # Creiamo una lista di frequenza dei gradi\n",
    "    freq= [ 0 for d in range(dmax) ]\n",
    "    for d in degseq:\n",
    "        freq[d] += 1\n",
    "    # Retituiamo la lista di frequenza, ordinata crescentemente\n",
    "    return freq\n",
    "\n",
    "# Disegna il confronto della distribuzione dei gradi\n",
    "def compare_degree_distribution(partial_G, total_G, in_degree=False, out_degree=False):\n",
    "    if (nx.is_directed(partial_G)):\n",
    "        # Disegniamo la distribuzione dell'in/out-degree per il grafo diretto\n",
    "        degree_freq_partial = degree_histogram_directed(partial_G, in_degree=in_degree, out_degree=out_degree)\n",
    "        degree_freq_total = degree_histogram_directed(total_G, in_degree=in_degree, out_degree=out_degree)\n",
    "        plt.loglog(range(len(degree_freq_partial)), degree_freq_partial, 'go', label='parziale')\n",
    "        plot = plt.loglog(range(len(degree_freq_total)), degree_freq_total, 'r^', label='totale')\n",
    "    else:\n",
    "        # Disegniamo la distribuzione dell'in/out-degree per il grafo indiretto\n",
    "        degree_freq_partial = nx.degree_histogram(partial_G)\n",
    "        degree_freq_total = nx.degree_histogram(total_G)\n",
    "        plt.loglog(range(len(degree_freq_partial)), degree_freq_partial,'go', label=\"parziale\")\n",
    "        plot = plt.loglog(range(len(degree_freq_total)), degree_freq_total,'r^', label=\"totale\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Grado')\n",
    "    plt.ylabel('Numero di nodi')\n",
    "    # Restituiamo il disegno della distribuzione\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i grafi\n",
    "partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "partial_pg = nx.node_link_graph(read_json(graph_folder+\"/some_users_preferential_graph.json\"))\n",
    "full_pg = nx.node_link_graph(read_json(graph_folder+\"/all_users_preferential_graph.json\"))\n",
    "\n",
    "# Fissiamo la dimensione dei grafici\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "# Mostriamo la distribuzione degli in-degree\n",
    "plt.subplot(1, 3, 1)\n",
    "compare_degree_distribution(partial_fg, full_fg, in_degree=True)\n",
    "plt.title(\"Distribuzione degli in-degree\")\n",
    "\n",
    "# Mostriamo la distribuzione degli out-degree\n",
    "plt.subplot(1, 3, 2)\n",
    "compare_degree_distribution(partial_fg, full_fg, out_degree=True)\n",
    "plt.title(\"Distribuzione degli out-degree\")\n",
    "\n",
    "# Mostriamo la distribuzione dei gradi\n",
    "plt.subplot(1, 3, 3)\n",
    "compare_degree_distribution(partial_pg, full_pg)\n",
    "plt.title(\"Distribuzione dei gradi\")\n",
    "\n",
    "plt.suptitle(\"Distribzione dei gradi nei grafi diretti\")\n",
    "plt.savefig(out_folder+\"/degree_distributions.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzazione della più grande componente fortemente connessa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ciascuno dei due grafi, si produce una visualizzazione statica del grafo con una colorazione rossa dei nodi appartenenti alla più grande componente fortemente connessa (componente gigante), nera per gli altri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restituisce il sottografo con la componente gigante\n",
    "def get_giant_component_subgraph(graph):\n",
    "    # Identifichiamo i nodi appartenenti alla componente gigante\n",
    "    nodes = []\n",
    "    if nx.is_directed(graph):\n",
    "        nodes = max(nx.strongly_connected_components(graph), key=len)\n",
    "    else:\n",
    "        # Non ha senso parlare di CFC in un grafo diretto, possiamo cercare la componente connessa\n",
    "        nodes = max(nx.connected_components(graph), key=len)\n",
    "    \n",
    "    # Restituiamo il sottografo corrispondente\n",
    "    return nx.subgraph(graph, nodes)\n",
    "\n",
    "# Disegna 'graph' evidenziando in rosso i nodi della componente gigante\n",
    "def highlight_giant_component(graph):\n",
    "    # Identifichiamo il sottografo della componente gigante\n",
    "    giant = get_giant_component_subgraph(graph)\n",
    "\n",
    "    # Mappiamo a ciascun nodo di 'graph' il colore appropriato\n",
    "    color_map=[]\n",
    "    for node in graph:\n",
    "        if node in giant.nodes:\n",
    "            # Se il nodo fa parte della SCC più grande viene colorato di rosso\n",
    "            color_map.append('red')\n",
    "        else:\n",
    "            # Altrimenti di nero\n",
    "            color_map.append('black')\n",
    "\n",
    "    # Disegnamo il grafo con la mappatura definita\n",
    "    plot = nx.draw_networkx(\n",
    "        graph,\n",
    "        pos = nx.spring_layout(graph), # algoritmo di Fruchterman-Reingold\n",
    "        node_size = 100,\n",
    "        node_color = color_map,\n",
    "        edge_color = (0,0,0,0.3), # gli archi sono trasparenti al fine di far risaltare i nodi\n",
    "        with_labels = False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Mostra un'immagine di confronto tra due componenti giganti di due grafi\n",
    "def compare_giant_components(partial_G, total_G, name):\n",
    "    # Ci assicuriamo che le immagini siano sufficientemente grandi\n",
    "    plt.subplots(figsize=(12,6))\n",
    "\n",
    "    # Disegnamo il grafo parziale\n",
    "    plt.subplot(1, 2, 1)\n",
    "    highlight_giant_component(partial_G)\n",
    "    plt.title(\"Grafo parziale\")\n",
    "\n",
    "    # Disegnamo il grafo totale\n",
    "    plt.subplot(1, 2, 2)\n",
    "    highlight_giant_component(total_G)\n",
    "    plt.title(\"Grafo totale\")\n",
    "\n",
    "    # Salviamo e mostriamo quanto disegnato\n",
    "    plt.savefig(out_folder+\"/\"+name+\".pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i grafi diretti\n",
    "partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "\n",
    "# Confrontiamo le componenti giganti dei due grafi\n",
    "compare_giant_components(partial_fg, full_fg, \"following_giant_component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vuole capire quanto sono grandi le due componenti giganti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i due grafi diretti\n",
    "partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "\n",
    "# Calcoliamo la grandezza (in nodi) della componente gigante\n",
    "dim_partial = len(list(get_giant_component_subgraph(partial_fg).nodes()))\n",
    "dim_full = len(list(get_giant_component_subgraph(full_fg).nodes()))\n",
    "\n",
    "# Mostriamo i valori\n",
    "print(f\"\\nDimensione SCC del grafo parziale: {dim_partial}\\nDimensione SCC del grafo totale: {dim_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i grafi indiretti\n",
    "partial_pg = nx.node_link_graph(read_json(graph_folder+\"/some_users_preferential_graph.json\"))\n",
    "full_pg = nx.node_link_graph(read_json(graph_folder+\"/all_users_preferential_graph.json\"))\n",
    "\n",
    "# Confrontiamo le componenti giganti dei due grafi\n",
    "compare_giant_components(partial_pg, full_pg, \"preferential_giant_component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distanze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo calcolare le seguenti distanze sui due grafi:\n",
    "\n",
    "- il centro;\n",
    "- il raggio;\n",
    "- la distanza media;\n",
    "- la distanza massima (diametro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il centro, il raggio e la distanza massima (diametro) dipendono dal valore dell'eccentricità: se questa è infinita, non è molto informativa sul grafo. Risolviamo questo problema sostituendo al grafo la componente gigante per queste metriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamo i grafi rilevanti\n",
    "partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "partial_pg = nx.node_link_graph(read_json(graph_folder+\"/some_users_preferential_graph.json\"))\n",
    "full_pg = nx.node_link_graph(read_json(graph_folder+\"/all_users_preferential_graph.json\"))\n",
    "\n",
    "# Computiamo le massime scc per i grafi diretti\n",
    "partial_scc = get_giant_component_subgraph(partial_fg)\n",
    "full_scc = get_giant_component_subgraph(full_fg)\n",
    "\n",
    "# Prepariamo un dizionario delle distanze\n",
    "distances = {}\n",
    "\n",
    "#centro\n",
    "distances[\"center\"] = [\n",
    "    list(nx.center(partial_scc)),\n",
    "    list(nx.center(full_scc)),\n",
    "    list(nx.center(partial_pg)),\n",
    "    list(nx.center(full_pg))\n",
    "]\n",
    "#raggio\n",
    "distances[\"radius\"] = [\n",
    "    nx.radius(partial_scc),\n",
    "    nx.radius(full_scc),\n",
    "    nx.radius(partial_pg),\n",
    "    nx.radius(full_pg)\n",
    "]\n",
    "#distanza media\n",
    "distances[\"avarage_distance\"] = [\n",
    "    nx.average_shortest_path_length(partial_fg),\n",
    "    nx.average_shortest_path_length(full_fg),\n",
    "    nx.average_shortest_path_length(partial_pg),\n",
    "    nx.average_shortest_path_length(full_pg)\n",
    "]\n",
    "#distanza massima \n",
    "distances[\"max_distance\"] = [\n",
    "    nx.diameter(partial_scc),\n",
    "    nx.diameter(full_scc),\n",
    "    nx.diameter(partial_pg),\n",
    "    nx.diameter(full_pg)\n",
    "]\n",
    "\n",
    "# Visualizziamo le distanze attraverso una tabella\n",
    "display(pd.DataFrame(distances, index=[\"partial_fg\", \"full_fg\", \"partial_pg\", \"full_pg\"]))\n",
    "serialize_json(data_folder, \"distances.json\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci interessa capire quanti sono i centri di ogni grafo/componente gigante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico i dati sulle distanze\n",
    "distances = read_json(data_folder+\"/distances.json\")\n",
    "centers = distances[\"center\"]\n",
    "\n",
    "# Stampo il numero di centri di ogni grafo\n",
    "for i in range(4):\n",
    "    print(len(centers[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo delle misure di centralità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo calcolare le seguenti metriche inerenti alla centralità dei due grafi:\n",
    "\n",
    "- centralità di betweenness;\n",
    "- centralità di prossimità;\n",
    "- centralità di grado (in entrata/uscita);\n",
    "- PageRank;\n",
    "- HITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restituisce la lista di valori di un dizionario\n",
    "def to_list(dict):\n",
    "    return list(dict.values())\n",
    "\n",
    "# Compone un dizionario con le metriche di centralità rilevanti\n",
    "def compute_centrality_metrics(graph):\n",
    "    metrics = {}\n",
    "    # Betweenness\n",
    "    metrics[\"betweenness\"] = to_list(nx.betweenness_centrality(graph))\n",
    "    # Closeness\n",
    "    metrics[\"closeness\"] = to_list(nx.closeness_centrality(graph))\n",
    "    # (in/out-degree)\n",
    "    if(nx.is_directed(graph)):\n",
    "        metrics[\"in_degree\"] = to_list(nx.in_degree_centrality(graph))\n",
    "        metrics[\"out_degree\"] = to_list(nx.out_degree_centrality(graph))\n",
    "    # degree\n",
    "    metrics[\"degree\"] = to_list(nx.degree_centrality(graph))\n",
    "    # PageRank\n",
    "    metrics[\"pagerank\"] = to_list(nx.pagerank(graph))\n",
    "    # HITS\n",
    "    metrics[\"hits_hubs\"] = to_list(nx.hits(graph)[0])\n",
    "    metrics[\"hits_authorities\"] = to_list(nx.hits(graph)[1])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo i quattro grafi\n",
    "partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "partial_pg = nx.node_link_graph(read_json(graph_folder+\"/some_users_preferential_graph.json\"))\n",
    "full_pg = nx.node_link_graph(read_json(graph_folder+\"/all_users_preferential_graph.json\"))\n",
    "\n",
    "# Calcoliamo le metriche di centralità per i quattro grafi\n",
    "partial_fg_centrality = compute_centrality_metrics(partial_fg)\n",
    "full_fg_centrality = compute_centrality_metrics(full_fg)\n",
    "partial_pg_centrality = compute_centrality_metrics(partial_pg)\n",
    "full_pg_centrality = compute_centrality_metrics(full_pg)\n",
    "\n",
    "# Mostriamo le metriche calcolate sotto forma di tabella\n",
    "print(\"Grafo diretto parziale\")\n",
    "display(pd.DataFrame(partial_fg_centrality, index=partial_fg.nodes()))\n",
    "print(\"Grafo diretto totale\")\n",
    "display(pd.DataFrame(full_fg_centrality, index=full_fg.nodes()))\n",
    "print(\"Grafo \\\"preferenziale\\\" parziale\")\n",
    "display(pd.DataFrame(partial_pg_centrality, index=partial_pg.nodes()))\n",
    "print(\"Grafo \\\"preferenziale\\\" totale\")\n",
    "display(pd.DataFrame(full_pg_centrality, index=full_pg.nodes()))\n",
    "\n",
    "# Salviamo i dati\n",
    "serialize_json(data_folder, \"partial_fg_centrality.json\", partial_fg_centrality)\n",
    "serialize_json(data_folder, \"full_fg_centrality.json\", full_fg_centrality)\n",
    "serialize_json(data_folder, \"partial_pg_centrality.json\", partial_pg_centrality)\n",
    "serialize_json(data_folder, \"full_pg_centrality.json\", full_pg_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riteniamo sia più comodo riassumere queste informazioni in modo da poter riuscire ad eseguire osservazioni sensate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grafo diretto parziale\")\n",
    "display(pd.DataFrame(partial_fg_centrality, index=partial_fg.nodes()).describe())\n",
    "print(\"Grafo diretto totale\")\n",
    "display(pd.DataFrame(full_fg_centrality, index=full_fg.nodes()).describe())\n",
    "print(\"Grafo \\\"preferenziale\\\" parziale\")\n",
    "display(pd.DataFrame(partial_pg_centrality, index=partial_pg.nodes()).describe())\n",
    "print(\"Grafo \\\"preferenziale\\\" totale\")\n",
    "display(pd.DataFrame(full_pg_centrality, index=full_pg.nodes()).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche se sono state riassunte, questi dati comunicano ancora troppo poco. Riteniamo sia più utile visualizzare graficamente queste informazioni. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disegna il grafo evvidenziando il valore di una misura con heatmap\n",
    "def draw_metric(G, pos, measures, measure_name):\n",
    "    # Disegnamo i nodi colorati mediante heatmap coerentemente al valore di 'measures'\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, node_size=50, cmap=plt.cm.plasma, \n",
    "                                   node_color=measures)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=(0,0,0,0.1)) # gli archi non sono essenziali\n",
    "    plt.title(measure_name)\n",
    "    plt.colorbar(nodes) # legenda della heatmap\n",
    "    plt.axis('off')\n",
    "    return nodes\n",
    "\n",
    "# Compara una metrica di centralità dei 4 grafi in esame\n",
    "def draw_metric_comparison(centrality_metric):\n",
    "    # Carichiamo i grafi con le rispettive metriche di centralità\n",
    "    partial_fg = nx.node_link_graph(read_json(graph_folder+\"/some_users_following_graph.json\"))\n",
    "    partial_fg_centrality = read_json(data_folder+\"/partial_fg_centrality.json\")\n",
    "    full_fg = nx.node_link_graph(read_json(graph_folder+\"/all_users_following_graph.json\"))\n",
    "    full_fg_centrality = read_json(data_folder+\"/full_fg_centrality.json\")\n",
    "    partial_pg = nx.node_link_graph(read_json(graph_folder+\"/some_users_preferential_graph.json\"))\n",
    "    partial_pg_centrality = read_json(data_folder+\"/partial_pg_centrality.json\")\n",
    "    full_pg = nx.node_link_graph(read_json(graph_folder+\"/all_users_preferential_graph.json\"))\n",
    "    full_pg_centrality = read_json(data_folder+\"/full_pg_centrality.json\")\n",
    "\n",
    "    # Fissiamo la dimensione dei grafi\n",
    "    plt.subplots(figsize=(10,10))\n",
    "\n",
    "    # Si mostra il grafo diretto parziale\n",
    "    plt.subplot(2, 2, 1)\n",
    "    draw_metric(\n",
    "        partial_fg, \n",
    "        nx.spring_layout(partial_fg), \n",
    "        partial_fg_centrality[centrality_metric], \n",
    "        'Grafo diretto parziale'\n",
    "    )\n",
    "\n",
    "    # Si mostra il grafo diretto totale\n",
    "    plt.subplot(2, 2, 2)\n",
    "    draw_metric(\n",
    "        full_fg, \n",
    "        nx.spring_layout(full_fg), \n",
    "        full_fg_centrality[centrality_metric], \n",
    "        'Grafo diretto totale'\n",
    "    )\n",
    "\n",
    "    # Si mostra il grafo indiretto parziale\n",
    "    plt.subplot(2, 2, 3)\n",
    "    draw_metric(\n",
    "        partial_pg, \n",
    "        nx.spring_layout(partial_pg), \n",
    "        partial_pg_centrality[centrality_metric], \n",
    "        'Grafo indiretto parziale'\n",
    "    )\n",
    "\n",
    "    # Si mostra il grafo indiretto totale\n",
    "    plt.subplot(2, 2, 4)\n",
    "    draw_metric(\n",
    "        full_pg, \n",
    "        nx.spring_layout(full_pg), \n",
    "        full_pg_centrality[centrality_metric], \n",
    "        'Grafo indiretto totale'\n",
    "    )\n",
    "\n",
    "    plt.suptitle(centrality_metric.capitalize()+\" Centrality\")\n",
    "    \n",
    "    # Salviamo e mostriamo il grafo\n",
    "    plt.savefig(out_folder+\"/\"+centrality_metric+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness\n",
    "draw_metric_comparison(\"betweenness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closeness\n",
    "draw_metric_comparison(\"closeness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grado\n",
    "draw_metric_comparison(\"degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PageRank\n",
    "draw_metric_comparison(\"pagerank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub di HITS\n",
    "draw_metric_comparison(\"hits_hubs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authorities di HITS\n",
    "draw_metric_comparison(\"hits_authorities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficenti per la stima della \"small-worldness\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I coefficienti per la \"small-worldness\" del grafo indiretto sono:\n",
    "\n",
    "- il coefficiente omega;\n",
    "- il coefficiente sigma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Li calcoliamo solo sui grafi indiretti perché ha poco senso parlare di reti piccolo-mondo in un grafo diretto (è più difficile realizzare l'ipotesi di grafo connesso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo i grafi indiretti\n",
    "partial_pg = nx.node_link_graph(read_json(graph_folder+\"/some_users_preferential_graph.json\"))\n",
    "full_pg = nx.node_link_graph(read_json(graph_folder+\"/all_users_preferential_graph.json\"))\n",
    "\n",
    "# Calcoliamo i valori\n",
    "small_worldness = {}\n",
    "small_worldness[\"omega\"] = [nx.omega(partial_pg), nx.omega(full_pg)]\n",
    "small_worldness[\"sigma\"] = [nx.sigma(partial_pg), nx.sigma(full_pg)]\n",
    "\n",
    "# Mostriamo i risultati in tabella\n",
    "display(pd.DataFrame(small_worldness, index=[\"partial_pg\", \"full_pg\"]))\n",
    "serialize_json(data_folder, \"small_worldness.json\", small_worldness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f523f7c76dd18e7ed336217f32f6f704c23c323644912475b9d3570cf04b060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
